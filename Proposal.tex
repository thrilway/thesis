%        File: Proposal.tex
%     Created: Thu Aug 25 12:00 PM 2016 E
% Last Change: Thu Aug 25 12:00 PM 2016 E
%
% arara: pdflatex
% arara: biber
% arara: pdflatex
% arara: pdflatex
\documentclass[letterpaper,12pt]{article}

\usepackage[
margin=1in
]{geometry}
\usepackage[backend=biber,style=authoryear-comp,useprefix=false]{biblatex}

\usepackage{stmaryrd}
\usepackage[]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{forest}
\usepackage{tabularx}
\usepackage{linguex}
\usepackage{centernot}
\usepackage{todonotes}

\useforestlibrary{linguistics}

\forestset{tree defaults/.style={for tree={parent anchor=south, child anchor=north},every tree node/.style={align=center,anchor=north},level/.style={sibling distance=50mm/#1},baseline}}

\forestset{en/.style={parent anchor=center, child anchor=center}}
\forestset{em/.style={parent anchor=north west, child anchor=north west}}
\forestset{el/.style={parent anchor=north, child anchor=north}}

\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}
%\DeclareNameFormat{labelname:poss}{% Based on labelname from biblatex.def
%  \ifcase\value{uniquename}%
%  \usebibmacro{name:last}{#1}{#3}{#5}{#7}%
%  \or
%  \ifuseprefix
%  {\usebibmacro{name:first-last}{#1}{#4}{#5}{#8}}
%  {\usebibmacro{name:first-last}{#1}{#4}{#6}{#8}}%
%  \or
%  \usebibmacro{name:first-last}{#1}{#3}{#5}{#7}%
%  \fi
%  \usebibmacro{name:andothers}%
%  \ifnumequal{\value{listcount}}{\value{liststop}}{'s}{}
%}
%
%\DeclareFieldFormat{shorthand:poss}{%
%  \ifnameundef{labelname}{#1's}{#1}
%}
%
%\DeclareFieldFormat{citetitle:poss}{\mkbibemph{#1}'s}
%
%\DeclareFieldFormat{label:poss}{#1's}
%
%\newrobustcmd*{\posscitealias}{%
%  \AtNextCite{%
%    \DeclareNameAlias{labelname}{labelname:poss}%
%    \DeclareFieldAlias{shorthand}{shorthand:poss}%
%    \DeclareFieldAlias{citetitle}{citetitle:poss}%
%    \DeclareFieldAlias{label}{label:poss}
%  }
%}
%
%\newrobustcmd*{\posscite}{%
%  \posscitealias%
%  \textcite
%}
%
%\newrobustcmd*{\Posscite}{\bibsentence\posscite}
%
%\newrobustcmd*{\posscites}{%
%  \posscitealias%
%  \textcites
%}

\newcommand\quelle[1]{{%
  \unskip\nobreak\hfil\penalty50
  \hskip2em\hbox{}\nobreak\hfil#1%
  \parfillskip=0pt \finalhyphendemerits=0 \par
}
}

\newcommand{\figex}{\refstepcounter{ExNo}\theExNo\hspace{\Exlabelsep}}

\newcommand{\hxp}{$\left\{ \text{X, YP} \right\}$}
\newcommand{\hh}{$\left\{ \text{X, Y} \right\}$}
\bibliography{Thesis}
\linespread{1.3}
\begin{document}

\section{Introduction}

The promise of generative syntax is that, given fixed innate grammatical principles, unlearnable and seemingly deep variation can be derived from learnable surface variation.
In current minimalist theories of syntax, the locus of variation is found in the lexicon, as expressed succinctly by \citeauthor{baker2008microparameter}'s (\citeyear{baker2008microparameter}) Chomsky-Borer Conjecture, given below in \Next.
\ex. The Borer-Chomsky Conjecture\\
All parameters of variation are attributable to differences in the features of particular items (e.g., the functional heads) in the lexicon. \hfill \parencite{baker2008microparameter}

This conjecture follows from various theoretical and empirical findings throughout the history of generative grammar, but is not fully explained.
Consider the correlation between obligatory predicative adjective agreement and the impossibility of adjectival resultatives \parencite{kratzer_building_2004}.
Compare French and German for instance.
In both languages, attributive adjective agree for number and gender with the nominals they modify. 
The languages differ with respect to predicative agreement, however.
German disallows agreement on predicative adjectives, while French requires it.
\ex. French 
\ag. la grand *(-e) femme\\
The tall \textsc{~~agr} woman(Fem)\\
``the tall woman''
\bg. La femme est grand *(-e).\\
the woman(fem) is tall \textsc{~~agr}\\
``The woman is tall.''
\z.

\ex. German 
\ag. die gro\ss{} *(-e) Frau\\
the tall \textsc{~~agr} woman\\
``the tall woman''
\bg. Die Frau ist gro\ss{} (*-e).\\
the woman(fem) is tall \textsc{~~agr}\\
``The woman is tall''
\z.

These patterns, being surface patterns, are, in principle, learnable from the primary linguistic data (PLD).

The languages also differ regarding whether they generate adjectival resultatives.
French (and other Romance languages) does not generate adjectival resultatives, while German (and other Germanic languages) does.
\ex. <+ResultativeExamples+> 

Since the variation demonstrated in \Last has to do with the meanings assigned to expressions rather than surface properties of the expressions, such variation is not, in principle, learnable from the PLD.

So, we have two types of variation (one shallow and learnable, one deep and unlearnable) which seem to be correlated with each other (predicative adjective agreement $\leftrightarrow$ *adjectival resultatives).
Given The Borer-Chomsky Conjecture, then, it is a reasonable hypothesis that the unlearnable variation follows from the learnable variation.
That is, the lack of predicative adjective agreement in German allows for adjectival resultatives and the presence of the former disallows the latter in French.

That such disparate phenomena are linked should surprise no-one familiar with minimalist theorizing.
How they might be linked, however, is the puzzle that this thesis occupies itself with.
Specifically, I will argue that Chomsky's (\citeyear{chomsky2013problems,chomsky2015problems}) recent label theory, taken to its logical conclusion, provides an account of how adjectival morphology and resultative constructions are linked.


\subsection{Previous Answers}
\subsubsection{Harley, Ramchand, Folli, et al.}
\begin{itemize}
  \item \textsc{Manner, Path, Place, Result}, \textit{etc.} features are lexicalized on different heads in different languages
  \item Basically a reiteration of Talmy
  \item May be true but not an account
\end{itemize}
\subsubsection{Snyder, Beck, Kratzer}
\begin{itemize}
  \item Compounding parameter determines V-framed/S-framed
  \item Similar in spirit to what I will propose
  \item Compounding parameter is likely unformulable in current theory
\end{itemize}

\section{Sharpening the Question}\label{sec:result analysis}
A more precise definition of resultatives is needed to proceed in this study.
Resultative clauses are clauses that contain two distinct predicates (Pred1$\neq$Pred2) which share an argument, in which the primary predicate is construed as the cause of the secondary.
Each of these properties that define resultatives (argument sharing and ``causativity'') is quite common and neither is sufficient for resultatives.
Depictives, which seem to occur in all of the worlds languages, such as those seen in \Next show argument sharing without ``causativity''.
\ex. \textbf{Depictives}
\a. Mary left angry. (Mary was angry.)
\b. Bill ate the fish raw. (The fish was raw.)
\b. Jamie swam the race naked. (Jamie was naked.)
\b. <+DepictivesInOtherLangs+>
\z.

So, the clause \textit{Mary left angry} means that the two eventualities, the event $e$ of Mary leaving and the state $s$ of Mary being angry, are stand in either an identity ($e=s$) or containment ($e\leq s$) relation rather than a causal relation.

``Causativity'', broadly construed, is even more prevalent in language.
Every instance of a sentence with an agent encodes ``causativity''.
For example, the clause \textit{John ironed the shirt} means that John acted in such a way as to cause the shirt to be ironed.

English-type languages, then, are those which generate clauses in which a single argument is shared between two predicates which are in a ``causative'' relation.
French-type languages, on the other hand, are those which do not generate clauses with both properties.
This means that an explanation of the parametric split between English- and French-type languages needs three components.
First, it requires a theory of argument sharing.
Second, must include a theory of how relations between predicates are established.
Finally, it must show that, for a given language, there is some component of the primary lingustic data which determines whether the two phenomena are compatible.

\subsection{What is Argument Sharing}
First, Lets consider cases of argument sharing, which, semantically speaking, is a situation in which one entity is interpreted as being a participant in multiple distinct events expressed by a single utterence.
A familiar type of argument sharing is control sentences such as \Next, in which Alice is both the holder of the wanting attitude and the agent of the non-actual winning event.
\ex.\label{ex:Control} Alice$_i$ wants $ec_i$ to win.

Other instances of argument sharing are seen in parasitic gaps, and depictives as in \Next and \NNext, respectively.

\ex. Who$_i$ did you discuss $ec_i$ without meeting $ec_i$?

\ex. Monica left angry.

The notion of $\Theta$-roles is the syntactic analogue to the predicate-argument relation in semantics.
If an entity is interpreted as the argument of a predicate, then the phrase that encodes that entity is $\Theta$-marked by the head/phrase that encodes the predicate.
Following a basic minimalist assumption -- TRAP \parencite{hornsteinetal2005understanding} -- $\Theta$-roles are assigned upon merge, and, following a more controversial assumption -- UTAH \parencite{baker1988incorporation}-- there is a mapping between $\Theta$-roles and syntactic positions.
This means that, for example, a DP is interpreted as the Theme of a verb iff it is merged as the complement of that verb.

I will be making two non-standard assumptions regarding $\Theta$-roles which, being non-standard, require justification.
First, I assume that DPs can receive multiple $\Theta$-roles, or in other words, I do not assume the $\Theta$ criterion.
There are two justifications for this assumption, one metatheoretical and the other theoretical.
The metatheoretical argument is based on the method of theory building prescribed by the minimalist program.
Theories are built by identifying virtual conceptual necessities (VCNs) and assuming those as axioms of the theory.
Any proposed principles or theoretical constructs are admited to the theory in one of two ways.
Either they are shown to follow logically from VCNs or they are argued to be VCNs.
In current syntactic theory, the set of VCNs is restricted to a lexicon, a structure building operation (merge) and the interfaces with other cognitive modules (sensorymotor and conceptual-intentional).
Since the $\Theta$-criterion is not a member of this set, its existence, rather than its nonexistence, must be argued for.
In other words, the burden of proof is on those who would propose the $\Theta$-criterion rather than those who reject it.

The $\Theta$-criterion was a constraint in GB syntax that held a D-Structure. 
It stated that every $\Theta$-role must be assigned to exactly one argument.
Since D-Structure has been eliminated from syntactic theory, if the $\Theta$-criterion does hold, it must hold at one of the interfaces, and since $\Theta$-roles are essentially semantic it must hold at the CI-interface.
So we must ask how the $\Theta$-criterion would be formulated as a constraint on CI legibility.

To do so, let's consider a concrete case.
Compare the two possible structures for an instance of control in \Next.
\ex.
\a. Richard$_i$ wanted [PRO$_i$ to laugh].
\b. Richard$_i$ wanted [$\langle$Richard$_i\rangle$ to laugh].

The structure in \Last[a], with PRO, obeys the $\Theta$-criterion, and the one in \Last[b] violates it, but both map to the same logical form, given in \Next.
\ex. $\exists e [\text{Wanted}(e)(\textbf{r}) \& \text{Laugh}(e) \& \textsc{Agent}(e)(\textbf{r})]$

The $\Theta$-criterion would then amount to a stipulation that \LLast[a] is good and \LLast[b] is bad, and for my task here, I see no use for such a stipulation.

The second non-standard assumption is in the particular version of UTAH I use.
\textcite{bakerXXXX} argues for the following mapping of $\Theta$-roles to structural position.
Agents are asssociated with Spec Voice, Themes with Spec V, and Goals with Comp V as in \Next, below.
\begin{figure}[h]
  \refstepcounter{ExNo}\theExNo\hspace{\Exlabelsep}\label{fig:BakerUTAH}
  {\small
\begin{forest}
  nice empty nodes,sn edges,baseline
  [VoiceP
    [\textsc{Agent}]
    [Voice$^\prime$
      [Voice]
      [VP
	[\textsc{Theme}]
	[V$^\prime$
	  [V]
	  [\textsc{Goal}]
	]
      ]
    ]
  ]
\end{forest}}
\end{figure}

<+FinishThisThought+>

This leads to a conception of argument sharing, according to which, DPs are shared arguments iff they are merged in two $\Theta$ positions in the course of a single derivation.
For ordinary control sentences, this is trivial to represent.
Consider \ref{ex:Control}, above, in which the subject \textit{Alice} bears two $\Theta$-roles: External argument of \textit{want} and external argument of \textit{win}.
Assuming external $\Theta$-roles are assigned to DPs merged in Spec-Voice, this means that \textit{Alice} was merged in two distinct Voice projections.
This can be attained by a derivation including only standard upward movement operations as shown below in \Next.
\begin{figure}[h]
  \figex
  {\small 
    \begin{forest}
  nice empty nodes,sn edges,baseline
  [VoiceP
    [Alice,name=wanter]
    [
      [Voice,name=want voice]
      [VP
	[want]
	[TP
	  [{$\langle\text{Alice}\rangle$},name=to subj]
	  [
	    [to]
	    [VoiceP
	      [{$\langle\text{Alice}\rangle$},name=winner]
	      [
		[Voice,name=win voice]
		[win]
	      ]
	    ]
	  ]
	]
      ]
    ]
  ]
  \draw [->,thick] (winner) to[out=south west, in=south] (to subj);
  \draw [->,thick] (to subj) to[out=south west, in=south] (wanter);
  %\draw [->,dashed] (win voice) to[out=west, in=south] node[below]  {\small $\Theta$} (winner);
\end{forest}}
\end{figure}

Other instances of argument sharing, however, require movement into internal argument positions which, assuming internal $\Theta$-roles are assigned in Comp V, requires sideward movement.
Consider the depictive sentence \textit{I ate the meat raw}, represented below in \Next.
\begin{figure}[h]
  \figex
  {\small
\begin{forest}
  nice empty nodes,sn edges,baseline
  [VoiceP
    [I]
    [
      [Voice]
      [VP
	[VP
	  [eat]
	  [DP[the meat,roof]]
	]
	[SC
	  [DP,[{$\langle\text{the meat}\rangle$},roof]]
	  [raw]
	]
      ]
    ]
  ]
\end{forest}}
\end{figure}
While sideward movement is not usually assumed to be allowed in merge-based derivations, certain interpretations of minimalist syntactic theory do allow it \parencite{nunes2001sideward,hornstein2009theory}.
I adapt these interpretations slightly to allow for the sideward movement necessary for movement to theme position.

Following \textcite{hornstein2009theory,nunes2001sideward}, I assume that in addition to linguistically proprietary operations, the faculty of language also uses domain general operations, specifically, a copying operation.
A copying operation, along with the necessary assumption that subtrees are derived separately before being merged together, gives us sideward movement.
To see how this works, consider the derivation of \Last (given in \Next, below).
Starting with the DP \textit{the meat} preconstructed, we build the small clause (a-c).
We then select the verb from the lexical array (c), copy the DP from the small clause (d), and merge the two to form the VP (e).
Finally, we merge the VP and the small clause (f) and we are left with a sideward movement structure.
\begin{table}
  \figex \textbf{Deriving \LLast}\\
  {\small
\begin{tabular}[t]{rll}
  & Lexical Array & Workspace\\
  (a) & $\left\{ \textit{raw, eat, \dots} \right\}$ & $\left\{ \left\{ \textit{the, meat} \right\} \right\}$\\
  \multicolumn{3}{l}{Select(raw)}\\
  (b) & $\left\{ \textit{eat, \dots} \right\}$ & $\left\{ \textit{raw}, \left\{\textit{the, meat}\right\} \right\}$ \\
  \multicolumn{3}{l}{Merge(raw, $\left\{ \textit{the, meat} \right\}$)}\\
  (c) & $\left\{  \textit{eat, \dots}\right\}$ & $\left\{ \left\{ \textit{raw}, \left\{\textit{the, meat}\right\} \right\} \right\}$ \\
  \multicolumn{3}{l}{Select(eat)}\\
  (d) & $\left\{ \textit{\dots} \right\}$ & $\left\{ \textit{eat}, \left\{ \textit{raw}, \left\{\textit{the, meat}\right\} \right\} \right\}$ \\
  \multicolumn{3}{l}{Copy($\left\{ \textit{the, meat} \right\}$)}\\
  (e) & $\left\{ \textit{\dots} \right\}$ &$\left\{ \textit{eat}, \left\{\textit{the, meat}\right\}, \left\{ \textit{raw}, \left\{\textit{the, meat}\right\} \right\} \right\}$ \\
  \multicolumn{3}{l}{Merge(eat,  $\left\{ \textit{the, meat} \right\}$)}\\
  (f) & $\left\{ \textit{\dots} \right\}$ &$\left\{ \left\{\textit{eat}, \left\{\textit{the, meat}\right\}\right\}, \left\{ \textit{raw}, \left\{\textit{the, meat}\right\} \right\} \right\}$ \\
  \multicolumn{3}{l}{Merge($\left\{\textit{eat}, \left\{\textit{the, meat}\right\}\right\}$, $\left\{ \textit{raw}, \left\{\textit{the, meat}\right\} \right\}$)}\\
  (g) & $\left\{ \ldots \right\}$ & $\left\{  \left\{ \left\{\textit{eat}, \left\{\textit{the, meat}\right\}\right\}, \left\{ \textit{raw}, \left\{\textit{the, meat}\right\} \right\} \right\}\right\}$ \\
  \multicolumn{3}{l}{\dots}
\end{tabular}}
\end{table}
As discussed by \textcite{nunes2001sideward}, the derivation in \Last and the structure it derives in \LLast would lead to a crash at PF due to a failure of copy deletion.
Assuming that, all else being equal, the higher copy of a syntactic object is pronounced and lower copies are deleted, a sideward movement structure ought to be unpronounceable.
If sideward movement is followed by movement to a position that c-commands both copies, then the copy deletion issues evaporate, and the higher copy is pronounced.

Returning to the specific example of object-oriented depictives, the preceding discussion means that the structure in \LLast cannot be the final structure of the sentence given, as such a structure is unpronounceable.
If we assume, following \textcite{lasnik1999minimalist}, that grammatical objects raise to Spec AgrO\footnote{It seems unlikely to me that there is a specialised grammatical category whose only property is that is licenses Object DPs. As such I assume AgrO to be some meaningful category, but I take no stance on what that category might be.} for abstract Case\footnote{I take abstract Case to be a phenomenon in need of explanation, rather than an explanation for a phenomenon.} licensing, then our sideward moved DP must further raise to and the issue dissolves.
\begin{figure}[h]
\figex
{\small
\begin{forest}
  nice empty nodes,sn edges,baseline
  [AgrOP
    [DP]
    [
      [AgrO]
      [VP
	[VP
	  [eat]
	  [{$\langle\text{DP}\rangle$}]
	]
	[SC
	  [{$\langle\text{DP}\rangle$}]
	  [raw]
	]
      ]
    ]
  ]
\end{forest}}
\end{figure}
To summarize, argument sharing is represented in the syntax by movement from one $\Theta$ position to another.
In canonical control constructions, this is a trivial upward movement operation.
In other argument sharing constructions, however, sideward movement is necessary which requires a further upward movement to a position that c-commands all other instances of the moved element.


\subsection{What is ``Causativity''}
Consider the causative-inchoative alternation demonstrated in \Next and \NNext.
\ex.\label{ex:inch} The toast burned.

\ex.\label{ex:caus} Paul burned the toast.

A theory of causativity is one that explains the intuition that \Last entails \LLast.
Broadly speaking, there are two types of theories of causativity: those that propose that \Last is, in some sense, derived from \LLast, that those that say it isn't.
For a derivational theory \parencite[e.g.][]{lakoff1976toward,hale1993argument,pietroski2003small}, there is one lexical entry, \textsc{burn} and two forms, \textit{burn}$_1$ and \textit{burn}$_2$, derived from it.
For a non-derivational theory \parencite[e.g.][]{fodor1970three}, there are two lexical entries, \textsc{burn}$_1$ and \textsc{burn$_2$}, which are stipulated to stand in an entailment relation ($x$ \textsc{burn}$_2\: y \rightarrow y$ \textsc{burn}$_1$) and are homonymous.

I will adopt a derivational theory based on the semantic analysis of causativity developed by \textcite{pietroski2003small}.\footnote{
  This analysis may be a notational variant of the analysis given by \textcite{kratzer_building_2004}
}
This analysis assumes a neo-davidsonian semantics, according to which \ref{ex:inch} describes an event of burning which the toast is the theme of, as formalized below in \Next.
\ex. $\exists e [\text{Burning}(e)\, \&\, \textsc{Theme}(e, \mathbf{the\_toast})]$

The causative alternant, describes a complex event which terminates in a toast burning event and has John as an agent, as formalized below in \Next.
\ex. $\exists e,f [ \textsc{Terminates}(f, e) \,\&\, \textsc{Agent}(f, \mathbf{paul}) \,\&\, \text{Burning}(e)\, \&\, \textsc{Theme}(e, \mathbf{the\_toast})]$

Pietroski defines termination as follows.
\begin{quote}
  [E]vent F \textit{terminates} event E, iff: E and F occur; F is a (perhaps improper) part of E; and F is an effect of every event that is a proper part of E but is not a part of F.\hfill\parencite[p190]{pietroski2003small}
\end{quote}

In order to discuss how causativity might be derived syntactically, let's consider the pieces required.
Starting with the inchoative as a basis, we need a verb and a theme argument.
In addition to that we need something that expresses the \textsc{Terminates} and \textsc{Agent} predicates, and the agent argument.
We further need to know how and where existential closure occurs.



\begin{itemize}
  \item Syntax of ``causativity''
    \begin{itemize}
      \item Generally in the verbal domain $\{\text{H, XP}\} \rightarrow $Causative
    \end{itemize}
\end{itemize}

(The above will largely be an excercise in making my assumptions explicit)

\section{Enter Resultatives}
Given the discussion above regarding the syntax and semantics of argument sharing and causativity, I will move on to discuss how the two notions interact to generate or fail to generate resultatives.
I will discuss resultatives using \Next as a representative example.
\ex. Jennifer hammered the metal flat

First, consider the analysis given by \textcite{kratzer_building_2004}, according to which, the theme \textit{the metal} and the adjective \textit{flat} form a small clause, which encodes a state description.
The small clause merges with result head, which encodes a causative relation between events, and the resulting resP is merged as the complement of the verb \textit{hammer}.
The small clause theme is then raised to Spec V where it is marked as the theme of \textit{hammer}, and from there the derivation proceeds as normal.
The vP this generates is given in \Next.
\begin{figure}[h]
\figex Kratzer's Resultative Structure\\
{\small
\begin{forest}
  nice empty nodes,sn edges,baseline
  [vP
    [v] 
    [
      [{the metal},name=V theme] 
      [VP
	[hammer] 
	[resP 
	  [res] 
	  [SC
	    [{$\langle\text{the metal}\rangle$},name=sc theme]
	    [flat]
	  ]
	]
      ]
    ]
  ]
  \draw[->] (sc theme) to[out=south west, in=south] (V theme);
\end{forest}}
\end{figure}

Semantically, this proposal depends on a standard montagovian theory \parencite[see][]{heimkratzer1998semantics} of composition and a neo-davidsonian event semantics with two novel notions: a refined definition of a \textsc{cause} predicate and a novel rule of composition.
The \textsc{cause} predicate defines a relation between events \textit{e} and \textit{f} where \textit{e} is an event of causing \textit{f}.
As Kratzer defines causality, the \textsc{cause} predicate is (likely) eqivalent to Pietroski's \textsc{Terminates} predicate, so no more needs to be said about it.

Kratzer requires a novel rule of composition because the resP (type $\langle s, t\rangle$) is neither of the same type as, nor in the domain of the verb (type $\langle e, \langle s, t\rangle\rangle$) it combines with.
To combine these objects, Kratzer uses the rule of \textit{event identification} \parencite{kratzer_severing_1996} as defined below in \Next.
\ex. \textbf{Event Identification}: If $\alpha$ is a branching node with daughters $\beta$ and $\gamma$, and if $\beta$ is of type $\langle e, \langle s, t\rangle\rangle$ and $\gamma$ is of type $\langle s, t\rangle$, then $\llbracket\alpha\rrbracket = \lambda x_e \lambda e_s [\llbracket\beta\rrbracket](x)(e) \& \llbracket\gamma\rrbracket(e)]$

Given the discussion above regarding argument sharing, however, I will modify Kratzer's analysis slightly.
In the modified analysis, the theme moves sideward from the small clause to Comp V, and the ResP merges with the VP, as shown below in \Next.
\begin{figure}[h]
\figex Kratzer + Sideward movement\\
{\small
  \begin{forest}
  nice empty nodes,sn edges,baseline
  [vP
    [v]
    [
      [VP
	[hammer]
	[DP[the metal,roof]]
      ]
      [resP
	[res]
	[SC
	  [DP[the metal,roof]]
	  [flat]
	]
      ]
    ]
  ]
\end{forest}}
\end{figure}

\begin{itemize}
  \item How does \Last[b] get a ``causative''  interpretation?
  \item How is \Last[b] ruled out in French-like languages?
\end{itemize}
\section{Label Theory}
<+IntroStuff+>
Label Theory begins with two principles: (i) Merge does not assign labels to the objects it creates, and (ii) labels are required for proper interpretation at the CI interface.
From these principles, Chomsky proposes that labels are assigned upon transfer (\textit{i.e.}, at the phase level) to the CI interface by a labeling algorithm (LA), which is a special case of minimal search that picks out the most prominent element contained in a syntactic object and assigns it as the label.
Given this conception of labeling, there are three relevant classes of syntactic objects: (i) Head-Phrase structures ($\left\{ \text{X, YP} \right\}$), (ii) Head-Head structures ($\left\{ \text{X, Y} \right\}$), and (iii) Phrase-Phrase structures ($\left\{ \text{XP, YP} \right\}$).\footnote{
  Technically speaking, there is a fourth class of syntactic objects, Heads, which I will not discuss for two reasons. First, because it is not clear that heads would require labels. And second, because if it required labelling, the labeling process would be trivial
}
The simplest case, Head-Phrase structures, require no special discussion.
The head is the most prominent constituent of the structure, so it becomes the label.
\ex. Label(\hxp) = X

The other structures, on the other hand are symmetric, and therefore do not necessarily have most prominent elements so the LA must use other factors to decide.
Head-Head structures can only be labeled if only one of the heads can be the label.
Chomsky notes that the only instances of licit (and therefore labelable) Head-Head structures are the result of merging a root with a category determining head, and proposes that this is because roots are simply unable to be labels.
\ex. Label(\hh) = X iff Y is a root and X is not a root.

The last, and most relevant to this dissertation, is the class of Phrase-Phrase structures, 
\section{Appendices}
\begin{itemize}
  \item 1st Generals Paper.
  \item Dog Days 2016 handout.
  \item Forum Paper? (perhaps tangentially relevant)
\end{itemize}
\printbibliography
\end{document}
