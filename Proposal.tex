%        File: Proposal.tex
%     Created: Thu Aug 25 12:00 PM 2016 E
% Last Change: Thu Aug 25 12:00 PM 2016 E
%
% arara: pdflatex
% arara: biber
% arara: pdflatex
% arara: pdflatex
\documentclass[letterpaper,12pt]{article}

\usepackage[
margin=1in
]{geometry}
\usepackage[backend=biber,style=authoryear-comp,useprefix=false]{biblatex}

\usepackage{stmaryrd}
\usepackage[]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{forest}
\usepackage{tabularx}
\usepackage{linguex}
\usepackage{centernot}
\usepackage{todonotes}
\usepackage{titling}
\useforestlibrary{linguistics}

\forestset{tree defaults/.style={for tree={parent anchor=south, child anchor=north},every tree node/.style={align=center,anchor=north},level/.style={sibling distance=50mm/#1},baseline}}

\forestset{en/.style={parent anchor=center, child anchor=center}}
\forestset{em/.style={parent anchor=north west, child anchor=north west}}
\forestset{el/.style={parent anchor=north, child anchor=north}}

\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}
%\DeclareNameFormat{labelname:poss}{% Based on labelname from biblatex.def
%  \ifcase\value{uniquename}%
%  \usebibmacro{name:last}{#1}{#3}{#5}{#7}%
%  \or
%  \ifuseprefix
%  {\usebibmacro{name:first-last}{#1}{#4}{#5}{#8}}
%  {\usebibmacro{name:first-last}{#1}{#4}{#6}{#8}}%
%  \or
%  \usebibmacro{name:first-last}{#1}{#3}{#5}{#7}%
%  \fi
%  \usebibmacro{name:andothers}%
%  \ifnumequal{\value{listcount}}{\value{liststop}}{'s}{}
%}
%
%\DeclareFieldFormat{shorthand:poss}{%
%  \ifnameundef{labelname}{#1's}{#1}
%}
%
%\DeclareFieldFormat{citetitle:poss}{\mkbibemph{#1}'s}
%
%\DeclareFieldFormat{label:poss}{#1's}
%
%\newrobustcmd*{\posscitealias}{%
%  \AtNextCite{%
%    \DeclareNameAlias{labelname}{labelname:poss}%
%    \DeclareFieldAlias{shorthand}{shorthand:poss}%
%    \DeclareFieldAlias{citetitle}{citetitle:poss}%
%    \DeclareFieldAlias{label}{label:poss}
%  }
%}
%
%\newrobustcmd*{\posscite}{%
%  \posscitealias%
%  \textcite
%}
%
%\newrobustcmd*{\Posscite}{\bibsentence\posscite}
%
%\newrobustcmd*{\posscites}{%
%  \posscitealias%
%  \textcites
%}

\newcommand\quelle[1]{{%
  \unskip\nobreak\hfil\penalty50
  \hskip2em\hbox{}\nobreak\hfil#1%
  \parfillskip=0pt \finalhyphendemerits=0 \par
}
}

\newcommand{\figex}{\refstepcounter{ExNo}\theExNo\hspace{\Exlabelsep}}

\newcommand{\hxp}{$\left\{ \text{X, YP} \right\}$}
\newcommand{\hh}{$\left\{ \text{X, Y} \right\}$}
\newcommand{\xpyp}{$\left\{ \text{XP, YP} \right\}$}
\bibliography{Thesis}
\linespread{1.3}

\title{Explaining the Resultative Parameter\\{\large Thesis Proposal}}
\author{Dan Milway}
\begin{document}
\maketitle

\section{Introduction}
The promise of generative syntax is that, given fixed innate grammatical principles, unlearnable and seemingly deep variation can be derived from learnable surface variation.
In current minimalist theories of syntax, the locus of variation is found in the lexicon, as expressed succinctly by \citeauthor{baker2008microparameter}'s (\citeyear{baker2008microparameter}) Borer-Chomsky Conjecture, given below in \Next.
\ex. The Borer-Chomsky Conjecture\\
All parameters of variation are attributable to differences in the features of particular items (e.g., the functional heads) in the lexicon. \hfill \parencite{baker2008microparameter}

This conjecture follows from various theoretical and empirical findings throughout the history of generative grammar, but is not fully explained.
Consider the correlation between obligatory predicative adjective agreement and the impossibility of adjectival resultatives \parencite{kratzer_building_2004}.
Compare French and German for instance.
In both languages, attributive adjective agree for number and gender with the nominals they modify. 
The languages differ with respect to predicative agreement, however.
German disallows agreement on predicative adjectives, while French requires it.
\ex. French 
\ag. la grand *(-e) femme\\
The tall \textsc{~~agr} woman(Fem)\\
``the tall woman''
\bg. La femme est grand *(-e).\\
the woman(fem) is tall \textsc{~~agr}\\
``The woman is tall.''
\z.

\ex. German 
\ag. die gro\ss{} *(-e) Frau\\
the tall \textsc{~~agr} woman\\
``the tall woman''
\bg. Die Frau ist gro\ss{} (*-e).\\
the woman(fem) is tall \textsc{~~agr}\\
``The woman is tall''
\z.

These patterns, being surface patterns, are, in principle, learnable from the primary linguistic data (PLD).

The languages also differ regarding whether they generate adjectival resultatives.
French (and other Romance languages) does not generate adjectival resultatives, while German (and other Germanic languages) does.
\ex. <+ResultativeExamples+> 

Since the variation demonstrated in \Last has to do with the meanings assigned to expressions rather than surface properties of the expressions, such variation is not, in principle, learnable from the PLD.

So, we have two types of variation (one shallow and learnable, one deep and unlearnable) which seem to be correlated with each other (predicative adjective agreement $\leftrightarrow$ *adjectival resultatives).
Given The Borer-Chomsky Conjecture, then, it is a reasonable hypothesis that the unlearnable variation follows from the learnable variation.
That is, the lack of predicative adjective agreement in German allows for adjectival resultatives and the presence of the former disallows the latter in French.

That such disparate phenomena are linked should surprise no-one familiar with minimalist theorizing.
How they might be linked, however, is the puzzle that this thesis occupies itself with.
Specifically, I will argue that Chomsky's (\citeyear{chomsky2013problems,chomsky2015problems}) recent label theory, taken to its logical conclusion, provides an account of how adjectival morphology and resultative constructions are linked.


\subsection{Previous Answers}
The puzzle of the resultative parameter has been approached from two directions.
One approach, found in the work of \textcite{harley2005how,folli2006licensing,ramchand2008verb,tungseth2008verbal}, among others, first asks how we might represent resultatives syntactically and then asks what a language's lexicon would have to look like to block the structures responsible for resultatives.
Such an approach also tends to assume lexical verbs are decomposable into primitive conceptual features/heads (\textsc{Process}, \textsc{Manner}, \textsc{Result}, \textsc{Path}, \textit{etc.}) in the spirit of \textcite{hale1993argument}.
In my first generals paper \parencite[][pp 30-32]{milway2015generals}, I argue that Ramchand's (\citeyear{ramchand2008verb}) particular proposal, which is perhaps the most fully realized and coherent proposals of this type, falls short of providing a syntactic analysis of the resultative parameter.
Furthermore, none of the work in this approach addresses the question of how such variation could be acquired, rather, they implicitly stipulate that it can be acquired.

The second approach to the resultative parameter, found in the work of \textcite{snyder1995language,snyder2001nature,roeper2002learnability,beck2001complex}, starts with the acquisition question.
In his 1995 dissertation, Snyder presents evidence from child language data that the acquisition of endocentric compounding is correlated with that of resultatives, and argues that they are both effects of a single syntactic parameter, given below in \Next.
\ex. \textbf{The Compounding Parameter} \parencite{snyder2001nature}\\ 
The grammar \{disallows*, allows\} formation of endocentric compounds during the syntactic derivation. [*unmarked value] 

While an interesting description, this work falls short of an explanation of the resultative parameter because it merely identifies a correlation between two phenomena but does not attempt to identify a causal link between them.
\textcite{kratzer_building_2004} attempts to describe such a link, but for reasons discussed in my first generals paper \parencite[][pp 32-34]{milway2015generals}, She falls short of explanatory adequacy.

My thesis will be an attempt to unify the two approaches. 
Such a unification, however, will require significant revision of the logic used in each approach.
I will discuss the revised logic in the following section.

\section{Towards an explanation of the resultative parameter}
In order to approach an explanation of this parametric variation, this thesis will be structured as two converging lines of inquiry.
The first line will be an attempt to formalize the surface variation associated with the resultative parameter.
\textcite{snyder2001nature,roeper2002learnability,kratzer_building_2004} and others identify the (un)availability of productive compunding as the relevant surface variation, while \textcite{haider2016predicting} identifies the (un)availability of particle verb constructions as the culprit.
My previous investigation of particle verbs \parencite[][included as an appendix]{milway2013forum} suggests that compunding and particle verbs are related, so I will assume that the compunding parameter subsumes the particle verb parameter.
This variation is typically formalized as parameters of the grammar, such as Snyder's Compounding Parameter below.
\ex. \textbf{Compounding Parameter} \parencite{snyder2001nature}\\ 
The grammar \{disallows*, allows\} formation of endocentric compounds during the syntactic derivation. [*unmarked value] 

Given the theory of grammar I assume, \Last would be considered a descriptive/typological statement in need of theoretical explanation.

The first step towards such an explanation is to rephrase \Last with the Borer-Chomsky Conjecture in mind.
If we consider an I-language, L, to be the pair of a variable lexicon, Lex$_\text{L}$, and a fixed Universal Grammar, UG, then we can restate The Compounding Parameter as in \Next.
\ex. \textbf{The Compounding Parameter} (revised):\\
The lexicon of an I-language, L, is such that principles of UG \{disallow*, allow\} formation of endocentric compounds during a syntactic derivation in L. [*unmarked value]

From this formulation, the next step is to investigate how endocentric compounds are derived syntactically and how they might be blocked.
If this investigation is done by holding the principles of UG constant, then we will have arrived at a characterization of those properties of the lexicon that are linked to TCP.

The second line of inquiry will investigate resultative structures directly, how they are derived, and how they are blocked.
As with the first line of inquiry, The goal of this line will be to identify the lexical properties that determine whether or not a given I-language generates resultatives.
By hypothesis, the properties identified by this line of inquiry should be identical to those identified by the first line of inquiry.


\begin{figure}[h]
  \figex
  \begin{tikzpicture}[baseline]
    \node (lex) at (0,-1) {Lexicon};
    \node (comp) at (4,0) {\{*\}Compounding};
    \node (res) at (4,-2) {\{*\}Resultatives};
    \node (pld) at (8,0) {PLD};
    \draw [<->] (lex)--(comp);
    \draw [<->] (lex)--(res);
    \draw [->] (pld)--(comp);
  \end{tikzpicture}
  \label{fig:ParameterModel}
\end{figure}

\section{Sharpening the Question}\label{sec:result analysis}
A more precise definition of resultatives is needed to proceed in this study.
Resultative clauses are clauses that contain two distinct predicates (Pred1$\neq$Pred2) which share an argument, in which the primary predicate is construed as the cause of the secondary.
Each of these properties that define resultatives (argument sharing and ``causativity'') is quite common and neither is sufficient for resultatives.
Depictives, which seem to occur in all of the worlds languages, such as those seen in \Next show argument sharing without ``causativity''.
\ex. \textbf{Depictives}
\a. Mary left angry. (Mary was angry.)
\b. Bill ate the fish raw. (The fish was raw.)
\b. Jamie swam the race naked. (Jamie was naked.)
\b. <+DepictivesInOtherLangs+>
\z.

So, the clause \textit{Mary left angry} means that the two eventualities, the event $e$ of Mary leaving and the state $s$ of Mary being angry, are stand in either an identity ($e=s$) or containment ($e\leq s$) relation rather than a causal relation.

``Causativity'', broadly construed, is even more prevalent in language.
Every instance of a sentence with an agent encodes ``causativity''.
For example, the clause \textit{John ironed the shirt} means that John acted in such a way as to cause the shirt to be ironed.

English-type languages, then, are those which generate clauses in which a single argument is shared between two predicates which are in a ``causative'' relation.
French-type languages, on the other hand, are those which do not generate clauses with both properties.
This means that an explanation of the parametric split between English- and French-type languages needs three components.
First, it requires a theory of argument sharing.
Second, must include a theory of how relations between predicates are established.
Finally, it must show that, for a given language, there is some component of the primary lingustic data which determines whether the two phenomena are compatible.

\subsection{What is Argument Sharing}
First, Lets consider cases of argument sharing, which, semantically speaking, is a situation in which one entity is interpreted as being a participant in multiple distinct events expressed by a single utterence.
A familiar type of argument sharing is control sentences such as \Next, in which Alice is both the holder of the wanting attitude and the agent of the non-actual winning event.
\ex.\label{ex:Control} Alice$_i$ wants $ec_i$ to win.

Other instances of argument sharing are seen in parasitic gaps, and depictives as in \Next and \NNext, respectively.

\ex. Who$_i$ did you discuss $ec_i$ without meeting $ec_i$?

\ex. Monica left angry.

The notion of $\Theta$-roles is the syntactic analogue to the predicate-argument relation in semantics.
If an entity is interpreted as the argument of a predicate, then the phrase that encodes that entity is $\Theta$-marked by the head/phrase that encodes the predicate.
Following a basic minimalist assumption -- TRAP \parencite{hornsteinetal2005understanding} -- $\Theta$-roles are assigned upon merge, and, following a more controversial assumption -- UTAH \parencite{baker1988incorporation}-- there is a mapping between $\Theta$-roles and syntactic positions.
This means that, for example, a DP is interpreted as the Theme of a verb iff it is merged as the complement of that verb.

I will be making two non-standard assumptions regarding $\Theta$-roles which, being non-standard, require justification.
First, I assume that DPs can receive multiple $\Theta$-roles, or in other words, I do not assume the $\Theta$ criterion.
There are two justifications for this assumption, one metatheoretical and the other theoretical.
The metatheoretical argument is based on the method of theory building prescribed by the minimalist program.
Theories are built by identifying virtual conceptual necessities (VCNs) and assuming those as axioms of the theory.
Any proposed principles or theoretical constructs are admited to the theory in one of two ways.
Either they are shown to follow logically from VCNs or they are argued to be VCNs.
In current syntactic theory, the set of VCNs is restricted to a lexicon, a structure building operation (merge) and the interfaces with other cognitive modules (sensorymotor and conceptual-intentional).
Since the $\Theta$-criterion is not a member of this set, its existence, rather than its nonexistence, must be argued for.
In other words, the burden of proof is on those who would propose the $\Theta$-criterion rather than those who reject it.

The $\Theta$-criterion was a constraint in GB syntax that held a D-Structure. 
It stated that every $\Theta$-role must be assigned to exactly one argument.
Since D-Structure has been eliminated from syntactic theory, if the $\Theta$-criterion does hold, it must hold at one of the interfaces, and since $\Theta$-roles are essentially semantic it must hold at the CI-interface.
So we must ask how the $\Theta$-criterion would be formulated as a constraint on CI legibility.

To do so, let's consider a concrete case.
Compare the two possible structures for an instance of control in \Next.
\ex.
\a. Richard$_i$ wanted [PRO$_i$ to laugh].
\b. Richard$_i$ wanted [$\langle$Richard$_i\rangle$ to laugh].

The structure in \Last[a], with PRO, obeys the $\Theta$-criterion, and the one in \Last[b] violates it, but both map to the same logical form, given in \Next.
\ex. $\exists e [\text{Wanted}(e)(\textbf{r}) \& \text{Laugh}(e) \& \textsc{Agent}(e)(\textbf{r})]$

The $\Theta$-criterion would then amount to a stipulation that \LLast[a] is good and \LLast[b] is bad, and for my task here, I see no use for such a stipulation.

The second non-standard assumption is in the particular version of UTAH I use.
\textcite{baker1988incorporation} argues for the following mapping of $\Theta$-roles to structural position.
Agents are asssociated with Spec Voice, Themes with Spec V, and Goals with Comp V as in \Next, below.
\begin{figure}[h]
  \refstepcounter{ExNo}\theExNo\hspace{\Exlabelsep}\label{fig:BakerUTAH}
  {\small
\begin{forest}
  nice empty nodes,sn edges,baseline
  [VoiceP
    [\textsc{Agent}]
    [Voice$^\prime$
      [Voice]
      [VP
	[\textsc{Theme}]
	[V$^\prime$
	  [V]
	  [\textsc{Goal}]
	]
      ]
    ]
  ]
\end{forest}}
\end{figure}
This analysis, however, assumed a theory of phrase structure which allows a phrase to have an empty complement but a filled specifier.
Consider the structure expected for a simple transitive with only an agent and a theme, given below in \Next.
\begin{figure}[h]
  \figex\label{fig:BakerUTAH}
  {\small
\begin{forest}
  nice empty nodes,sn edges,baseline
  [VoiceP
    [\textsc{Agent}]
    [Voice$^\prime$
      [Voice]
      [VP
	[\textsc{Theme}]
	[V$^\prime$
	  [V]
	]
      ]
    ]
  ]
\end{forest}}
\end{figure}
In a merge-based syntax, such a structure is impossible, as every instance of merge combines exactly two items, thus ruling out non-branching nodes.
The structure we would expect would place the theme as the complement of V, which would have no specifier, thus violating Baker's UTAH.
<+FinishThis+>

This leads to a conception of argument sharing, according to which, DPs are shared arguments iff they are merged in two $\Theta$ positions in the course of a single derivation.
For ordinary control sentences, this is trivial to represent.
Consider \ref{ex:Control}, above, in which the subject \textit{Alice} bears two $\Theta$-roles: External argument of \textit{want} and external argument of \textit{win}.
Assuming external $\Theta$-roles are assigned to DPs merged in Spec-Voice, this means that \textit{Alice} was merged in two distinct Voice projections.
This can be attained by a derivation including only standard upward movement operations as shown below in \Next.
\begin{figure}[h]
  \figex
  {\small 
    \begin{forest}
  nice empty nodes,sn edges,baseline
  [VoiceP
    [Alice,name=wanter]
    [
      [Voice,name=want voice]
      [VP
	[want]
	[TP
	  [{$\langle\text{Alice}\rangle$},name=to subj]
	  [
	    [to]
	    [VoiceP
	      [{$\langle\text{Alice}\rangle$},name=winner]
	      [
		[Voice,name=win voice]
		[win]
	      ]
	    ]
	  ]
	]
      ]
    ]
  ]
  \draw [->,thick] (winner) to[out=south west, in=south] (to subj);
  \draw [->,thick] (to subj) to[out=south west, in=south] (wanter);
  %\draw [->,dashed] (win voice) to[out=west, in=south] node[below]  {\small $\Theta$} (winner);
\end{forest}}
\end{figure}

Other instances of argument sharing, however, require movement into internal argument positions which, assuming internal $\Theta$-roles are assigned in Comp V, requires sideward movement.
Consider the depictive sentence \textit{I ate the meat raw}, represented below in \Next.
\begin{figure}[h]
  \figex
  {\small
\begin{forest}
  nice empty nodes,sn edges,baseline
  [VoiceP
    [I]
    [
      [Voice]
      [VP
	[VP
	  [eat]
	  [DP[the meat,roof]]
	]
	[SC
	  [DP,[{$\langle\text{the meat}\rangle$},roof]]
	  [raw]
	]
      ]
    ]
  ]
\end{forest}}
\end{figure}
While sideward movement is not usually assumed to be allowed in merge-based derivations, certain interpretations of minimalist syntactic theory do allow it \parencite{nunes2001sideward,hornstein2009theory}.
I adapt these interpretations slightly to allow for the sideward movement necessary for movement to theme position.

Following \textcite{hornstein2009theory,nunes2001sideward}, I assume that in addition to linguistically proprietary operations, the faculty of language also uses domain general operations, specifically, a copying operation.
A copying operation, along with the necessary assumption that subtrees are derived separately before being merged together, gives us sideward movement.
To see how this works, consider the derivation of \Last (given in \Next, below).
Starting with the DP \textit{the meat} preconstructed, we build the small clause (a-c).
We then select the verb from the lexical array (c), copy the DP from the small clause (d), and merge the two to form the VP (e).
Finally, we merge the VP and the small clause (f) and we are left with a sideward movement structure.
\begin{table}
  \figex \textbf{Deriving \LLast}\\
  {\small
\begin{tabular}[t]{rlll}
  & Lexical Array & Workspace\\
  (a) & 
  $\left\{ \textit{raw, eat, \dots} \right\}$ &
  $\left\{ \left[_\alpha \textit{the, meat} \right] \right\}$ &
  Select(raw)\\
  (b) &
  $\left\{ \textit{eat, \dots} \right\}$ &
  $\left\{ \textit{raw}, \left[_\alpha\textit{the, meat}\right] \right\}$ &
  Merge(raw, $\alpha$)\\
  (c) &
  $\left\{  \textit{eat, \dots}\right\}$ &
  $\left\{ \left[_\beta \textit{raw}, \left[_\alpha\textit{the, meat}\right] \right] \right\}$ &
  Select(eat)\\
  (d) &
  $\left\{ \textit{\dots} \right\}$ &
  $\left\{ \textit{eat}, \left[_\beta \textit{raw}, \left[_\alpha\textit{the, meat}\right] \right] \right\}$ &
  Copy($\alpha$)\\
  (e) &
  $\left\{ \textit{\dots} \right\}$ &
  $\left\{ \textit{eat}, \left[_\gamma\textit{the, meat}\right], \left[_\beta \textit{raw}, \left[_\alpha\textit{the, meat}\right] \right] \right\}$ &
  Merge(eat,  $\gamma$)\\
  (f) &
  $\left\{ \textit{\dots} \right\}$ &
  $\left\{ \left[_\delta\textit{eat}, \left[_\gamma\textit{the, meat}\right]\right], \left[_\beta \textit{raw}, \left[_\alpha\textit{the, meat}\right] \right] \right\}$ &
  Merge($\delta, \beta$)\\
  (g) &
  $\left\{ \ldots \right\}$ &
  $\left\{  \left[ _\zeta \left[_\delta\textit{eat}, \left[_\gamma\textit{the, meat}\right]\right], \left[_\beta \textit{raw}, \left[_\alpha\textit{the, meat}\right] \right] \right]\right\}$ &
  \\
\end{tabular}}
\end{table}
As discussed by \textcite{nunes2001sideward}, the derivation in \Last and the structure it derives in \LLast would lead to a crash at PF due to a failure of copy deletion.
Assuming that, all else being equal, the higher copy of a syntactic object is pronounced and lower copies are deleted, a sideward movement structure ought to be unpronounceable.
If sideward movement is followed by movement to a position that c-commands both copies, then the copy deletion issues evaporate, and the higher copy is pronounced.

Returning to the specific example of object-oriented depictives, the preceding discussion means that the structure in \LLast cannot be the final structure of the sentence given, as such a structure is unpronounceable.
If we assume, following \textcite{lasniksaito1999subject}, that grammatical objects raise to Spec AgrO\footnote{It seems unlikely to me that there is a specialised grammatical category whose only property is that is licenses Object DPs. As such I assume AgrO to be some meaningful category, but I take no stance on what that category might be.} for abstract Case\footnote{I take abstract Case to be a phenomenon in need of explanation, rather than an explanation for a phenomenon.} licensing, then our sideward moved DP must further raise to and the issue dissolves.
\begin{figure}[h]
\figex
{\small
\begin{forest}
  nice empty nodes,sn edges,baseline
  [AgrOP
    [DP]
    [
      [AgrO]
      [VP
	[VP
	  [eat]
	  [{$\langle\text{DP}\rangle$}]
	]
	[SC
	  [{$\langle\text{DP}\rangle$}]
	  [raw]
	]
      ]
    ]
  ]
\end{forest}}
\end{figure}
To summarize, argument sharing is represented in the syntax by movement from one $\Theta$ position to another.
In canonical control constructions, this is a trivial upward movement operation.
In other argument sharing constructions, however, sideward movement is necessary which requires a further upward movement to a position that c-commands all other instances of the moved element.


\subsection{What is ``Causativity''}
Consider the causative-inchoative alternation demonstrated in \Next and \NNext.
\ex.\label{ex:inch} The toast burned.

\ex.\label{ex:caus} Paul burned the toast.

A theory of causativity is one that explains the intuition that \Last entails \LLast.
Broadly speaking, there are two types of theories of causativity: those that propose that \Last is, in some sense, derived from \LLast, that those that say it isn't.
For a derivational theory \parencite[e.g.][]{lakoff1976toward,hale1993argument,pietroski2003small}, there is one lexical entry, \textsc{burn} and two forms, \textit{burn}$_1$ and \textit{burn}$_2$, derived from it.
For a non-derivational theory \parencite[e.g.][]{fodor1970three}, there are two lexical entries, \textsc{burn}$_1$ and \textsc{burn$_2$}, which are stipulated to stand in an entailment relation ($x$ \textsc{burn}$_2\: y \rightarrow y$ \textsc{burn}$_1$) and are homonymous.

I will adopt a derivational theory based on the semantic analysis of causativity developed by \textcite{pietroski2003small}.\footnote{
  This analysis may be a notational variant of the analysis given by \textcite{kratzer_building_2004}
}
This analysis assumes a Neo-Davidsonian semantics, according to which \ref{ex:inch} describes an event of burning which the toast is the theme of, as formalized below in \Next.
\ex. $\exists e [\text{Burning}(e)\, \&\, \textsc{Theme}(e, \mathbf{the\_toast})]$

The causative alternant, describes a complex event which terminates in a toast burning event and has John as an agent, as formalized below in \Next.
\ex. $\exists e,f [ \textsc{Terminates}(f, e) \,\&\, \textsc{Agent}(f, \mathbf{paul}) \,\&\, \text{Burning}(e)\, \&\, \textsc{Theme}(e, \mathbf{the\_toast})]$

Pietroski defines termination as follows.
\begin{quote}
  [E]vent F \textit{terminates} event E, iff: E and F occur; F is a (perhaps improper) part of E; and F is an effect of every event that is a proper part of E but is not a part of F.\hfill\parencite[p190]{pietroski2003small}
\end{quote}

In order to discuss how causativity might be derived syntactically, let's consider the pieces required.
Starting with the inchoative as a basis, we need a verb and a theme argument.
In addition to that we need something that expresses the \textsc{Terminates} and \textsc{Agent} predicates, and the agent argument.

I will assume that the inchoative verb is formed by merging a root with a category determining head.
This complex merges with and $\Theta$-marks the theme.
The fact that causatives can be passivized, as demonstrated below in \Next, suggests that the \textsc{Terminates} and \textsc{Agent} predicates are not (always) bundled together in a single head.
\ex. The toast was burned.
\a. $\neq$ The toast burned.
\b. $\neq$ John burned the toast.


\section{Enter Resultatives}
Given the discussion above regarding the syntax and semantics of argument sharing and causativity, I will move on to discuss how the two notions interact to generate or fail to generate resultatives.
I will discuss resultatives using \Next as a representative example.
\ex. Jennifer hammered the metal flat

First, consider the analysis given by \textcite{kratzer_building_2004}, according to which, the theme \textit{the metal} and the adjective \textit{flat} form a small clause, which encodes a state description.
The small clause merges with result head, which encodes a causative relation between events, and the resulting resP is merged as the complement of the verb \textit{hammer}.
The small clause theme is then raised to Spec V where it is marked as the theme of \textit{hammer}, and from there the derivation proceeds as normal.
The vP this generates is given in \Next.
\begin{figure}[h]
\figex Kratzer's Resultative Structure\\
{\small
\begin{forest}
  nice empty nodes,sn edges,baseline
  [vP
    [v] 
    [
      [{the metal},name=V theme] 
      [VP
	[hammer] 
	[resP 
	  [res] 
	  [SC
	    [{$\langle\text{the metal}\rangle$},name=sc theme]
	    [flat]
	  ]
	]
      ]
    ]
  ]
  \draw[->] (sc theme) to[out=south west, in=south] (V theme);
\end{forest}}
\end{figure}

Semantically, this proposal depends on a standard montagovian theory \parencite[see][]{heimkratzer1998semantics} of composition and a neo-davidsonian event semantics with two novel notions: a refined definition of a \textsc{cause} predicate and a novel rule of composition.
The \textsc{cause} predicate defines a relation between events \textit{e} and \textit{f} where \textit{e} is an event of causing \textit{f}.
As Kratzer defines causality, the \textsc{cause} predicate is (likely) eqivalent to Pietroski's \textsc{Terminates} predicate, so no more needs to be said about it.

Kratzer requires a novel rule of composition because the resP (type $\langle s, t\rangle$) is neither of the same type as, nor in the domain of the verb (type $\langle e, \langle s, t\rangle\rangle$) it combines with.
To combine these objects, Kratzer uses the rule of \textit{event identification} \parencite{kratzer_severing_1996} as defined below in \Next.
\ex. \textbf{Event Identification}: If $\alpha$ is a branching node with daughters $\beta$ and $\gamma$, and if $\beta$ is of type $\langle e, \langle s, t\rangle\rangle$ and $\gamma$ is of type $\langle s, t\rangle$, then $\llbracket\alpha\rrbracket = \lambda x_e \lambda e_s [\llbracket\beta\rrbracket](x)(e) \& \llbracket\gamma\rrbracket(e)]$

Given the discussion above regarding argument sharing, however, I will modify Kratzer's analysis slightly.
In the modified analysis, the theme moves sideward from the small clause to Comp V, and the ResP merges with the VP, as shown below in \Next.
\begin{figure}[h]
\figex Kratzer + Sideward movement\\
{\small
  \begin{forest}
  nice empty nodes,sn edges,baseline
  [vP
    [v]
    [
      [VP
	[hammer]
	[DP[the metal,roof]]
      ]
      [resP
	[res]
	[SC
	  [DP[the metal,roof]]
	  [flat]
	]
      ]
    ]
  ]
\end{forest}}
\end{figure}


In this thesis I will investigate the structure in \Last, asking how it might be barred in one language or another.
I will do so by comparing resultative structures with structures such as depictives which seem to be found in all of the worlds languages.
Given modern syntactic theory, according to which the narrow syntactic module is purely free merge, the fact that structures such as \Last are allowed or disallowed in a given language requires a refined theory of the interfaces to explain.
In the following section, I argue that Label Theory \parencite{chomsky2013problems,chomsky2015problems} provides the framework for such a theory.

\section{Label Theory}
In this thesis, I will be assuming and extending Chomsky's (\citeyear{chomsky2013problems,chomsky2015problems}) Label Theory, which begins with two principles: (i) Merge does not assign labels to the objects it creates, and (ii) labels are required for proper interpretation at the CI interface.
From these principles, Chomsky proposes that labels are assigned upon transfer (\textit{i.e.}, at the phase level) to the CI interface by a labeling algorithm (LA), which is a special case of minimal search that picks out the most prominent element contained in a syntactic object and assigns it as the label.
Given this conception of labeling, there are three relevant classes of syntactic objects: (i) Head-Phrase structures ($\left\{ \text{X, YP} \right\}$), (ii) Head-Head structures ($\left\{ \text{X, Y} \right\}$), and (iii) Phrase-Phrase structures ($\left\{ \text{XP, YP} \right\}$).\footnote{
  Technically speaking, there is a fourth class of syntactic objects, Heads, which I will not discuss for two reasons. First, because it is not clear that heads would require labels. And second, because if it required labelling, the labeling process would be trivial
}
The simplest case, Head-Phrase structures, require no special discussion.
The head is the most prominent constituent of the structure, so it becomes the label.
\ex. Label(\hxp) = X

The other structures, on the other hand are symmetric, and therefore do not necessarily have most prominent elements so the LA must use other factors to decide.
Head-Head structures can only be labeled if only one of the heads can be the label.
Chomsky notes that the only instances of licit (and therefore labelable) Head-Head structures are the result of merging a root with a category determining head, and proposes that this is because roots are simply unable to be labels.
\ex. Label(\hh) = X iff Y is a root and X is not a root.

The last, and most relevant to this dissertation, is the class of Phrase-Phrase structures.
Like the Head-Head structures, these are symmetrical, so there is no single most prominent element to be found.
Since every instance of internal merge and those instances of externally merging arguments, creates a Phrase-Phrase structure, there must be some way of labeling these structures.
Chomsky proposes two labeling strategies: (i) Lower copies of phrases are invisible to labeling, and (ii) if the two phrases agree for some feature, that feature can act as a label.
\ex. Label(\xpyp) = $
\begin{cases}
  \text{Label(YP)} & \text{iff XP is a lower copy}\\
  \langle\text{F,F}\rangle & \text{iff XP and YP agree for F}
\end{cases}
$

Recall that Chomsky proposes that labeling is a requirement of the CI interface.
It should be (and has been) noted, however, that, given our current conception of the CI interface, there is no reason to think that labelling is required for semantic interpretation \parencite[see \textit{e.g.,}][]{hornstein2016arizona}.
If a Phrase-Phrase structure has the proper semantic types, then why does it need a label?
Our current conception of the CI interface, however, is based on questionable assumptions regarding the status of natural language as a formal language, and the role of truth-conditions and reference in natural language meaning \parencite{chomsky2000new,pietroski2005events,moravcsik1998meaning}.
I, therefore, take the nature of the CI Interface to be an open question, to which Chomsky's label theory offers a partial answer.
Label theory, then, is not a theory of the Narrow Syntax, so much as it is a theory of the CI interface.

Beyond the theoretical arguments in label theory's favour, there are practical reasons to believe it will provide an explanation of the parametric variation at issue in this thesis.
Older theories of movement, such as Last Resort, offered conditions on movement \textit{to} a given structural postion, but no conditions of movement \textit{from} a given position.\footnote{This definitely needs to be fleshed out.}
The observation by \textcite[][and elsewhere]{rizzi2015notes} that phrases cannot move from so-called criterial positions, however, suggests that our theory of movement must include conditions on movement \textit{from}, in addition to conditions on movement \textit{to}.
\textcite{chomsky2015problems} presents an account of criterial freezing in terms of labeling, and thus shows that label theory offers conditions on movement \textit{from}.

\section{Preliminary Explanation}


\section{Appendices}
\begin{itemize}
  \item 1st Generals Paper.
  \item Dog Days 2016 handout.
  \item Forum Paper? (perhaps tangentially relevant)
\end{itemize}
\printbibliography
\end{document}
