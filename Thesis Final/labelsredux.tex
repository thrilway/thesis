%        File: labelsredux.tex
%     Created: Tue Jun 06 03:00 PM 2017 E
% Last Change: Tue Jun 06 03:00 PM 2017 E
%
% arara: pdflatex: {options: "-draftmode"}
% arara: biber
% arara: pdflatex: {options: "-draftmode"}
% arara: pdflatex: {options: "-file-line-error-style"}
\documentclass[MilwayThesis]{subfiles}

\begin{document}
In this section I will address two questions which \textcite{chomsky2013problems,chomsky2015problems} largely leaves open.
First there is the question of how to label Host-Adjunct structures.
These are cases of XP-YP structures, but generally involve neither movement of host or adjunct, nor agreement between the two.
Chomsky's LA, then, would crash when processing these structures.
I propose, following \textcite{hornstein2009theory} and \textcite{chametzky1996theory}, that Host-Adjunct structures are unlabelled.
How is it that a structure can be unlabelled?
To answer this question we must consider the nature of the labelling process. 

If we consider the labelling process to be a function Label from unlabelled structures to labelled structures, then an unlabelled structure is an impossible output of Label.
If Host-Adjunct structures are unlabelled, then they cannot be the output of Label.
This means one of two things, either Host-Adjunct structures are not interpreted at the CI interface, or they bypass Label.
The first possibility seems to be contradicted by the fact that Adjuncts are interpreted at CI, so I will follow the second possibility.
Consider, then, a structure [XP, ZP], where XP is a host and ZP is an adjunct.
The structure as a whole will bypass Label, but the host XP will be labelled.
It is reasonable to assume, then, that the adjunct ZP will bypass Label, meaning its internal structure will not be labelled in this cycle of Label.\footnote{The astute observer will likely note that this hypothesis contains a significant amount of stipulation.
I address this stipulation in chapter \ref{sec:Conclusion}}

The second question is why labels should be required by the CI interface at all.
My proposed answer is that the label of a complex object determines how that object composes semantically.
While this may seem \textit{ad hoc}, it is actually a quite reasonable hypothesis.
Consider Chomsky's labelling hypothesis as phrased in \Next, and the more standard theory of the CI interface in \NNext.
\ex. A syntactic object is a valid CI object iff it is labellable.

\ex. A syntactic object is a valid CI object iff it composes semantically.

At first glance, these hypotheses are incompatible, giving us three options at resolving the conflict.
The first option would be to reject one of the conflicting hypotheses.
There is no strong evidence, however, to reject either \LLast or \Last, so I will not choose this option.
The second option is to conjoin the iff clauses as in \Next.
\ex. A syntactic object is a valid CI object iff it is labellable and it composes semantically.

This option is unattractive for reasons of theoretical parsimony, so I will not choose it.
The third option is to hypothesize that labelling and composition are identical, and therefore the conflicting hypotheses are equivalent.
We can, then, replace our two conflicting statements with the two compatible statements in \Next and \NNext below.
\ex. A syntactic object composes iff it is labellable.

\ex. A syntactic object is a valid CI object iff it composes.

This move is theoretically attractive partially due to the fact that it mirrors the logic of antisymmetry on the SM interface \parencite{kayne1994antisymmetry}.
In the case of antisymmetry, Kayne identifies asymmetric c-command with linear order, and there is no compelling reason to think that the CI interface should be more complex than the SM interface.

So, what would it mean for labelling and composition to be two sides of the same coin?
Again, it is helpful to consider the SM interface, where asymmetric c-command and linear order are associated because they are isomorphic.
We should expect a similar isomorphism to hold between composition and labels, and, in fact, there seems to be good reason to think that there is such an isomorphism.
Consider the main modes of composition generally assumed by semanticists \parencite[\textit{e.g.}, by][]{heimkratzer1998semantics}, given schematically in \Next.
\ex. 
\a. \textbf{Lexical insertion}\\
\textsc{sem}($\alpha$) = $\alpha^\prime$
\b. \textbf{Function application}\\
\textsc{sem}($\left[ \alpha, \beta \right]$) = \textsc{sem}($\alpha$)(\textsc{sem}($\beta$))
\b. \textbf{Predicate modification}\\
\textsc{sem}($\left[ \alpha, \beta \right]$) = \textsc{sem}$(\alpha)(x) \&$ \textsc{sem}$(\beta)(x)$
\b. \textbf{Predicate abstraction}\\
\textsc{sem}($\left[ \alpha, \beta \right]$) = (Op$x$)(\textsc{sem}($\beta$)($x$))

Each of these modes of composition has a corresponding structure type as identified by label theory, including my modifications thus far.
Lexical insertion operates on a single syntactic atom, \textit{i.e.}, a head, which label theory necessarily distinguishes from other syntactic objects.
Predicate modification is the next most complex operation, it conjoins two (possibly complex) objects without requiring or inducing any ordering of the two, exactly isomorphic with the output of merge: unlabelled and unordered syntactic objects.
Function application, likewise, requires two objects, but these objects are ordered.
Unlike conjunction structures created by predicate modification, which are commutative ($X \& Y = Y \& X$), the function-argument structures created by function application are inherently asymmetric ($X(Y) \neq Y(X)$).
This matches with head-labelled stuctures, which encode a pair of objects (the contents of the structure) and an ordering statement (the label).
Finally, predicate abstraction, which creates structures similar to quantifier structures, requires the content of the two expressions, an ordering between the two, and a variable.
Pair-labelled structures provide this information.

The simplest cases are those structures labelled by heads.
The classes of structures which get head labels are given in \Next.
\ex. \textbf{Head-labelled structures}
\a. $\left\{ \text{X, }\textsc{Root} \right\} \xrightarrow{Label} \left[_\text{X} \text{X, }\textsc{root}  \right]$
\b. $\left\{ \text{X, YP} \right\} \xrightarrow{Label} \left[_\text{X} \text{X, YP} \right]$
\c. $\left\{ t_\text{ZP}, \left\{ \text{X, YP} \right\} \right\}\xrightarrow{Label}\left[_\text{X} t_\text{ZP}, \text{XP} \right]$

I propose that in these cases, the objects compose by function application, with the label being the function and the non-labelling constituent being the argument.
So, for instance, a DP is interpreted as the function D, with NP as an argument.
\ex. \textsc{sem}($\left[_\textit{the} \textit{the, ball} \right]$) = \textsc{sem}(\textit{the})(\textsc{sem}(\textit{ball}))

The next case is that of structures labelled by feature-pairs.
These structures, tend to be the result of internal Merge, which is generally associated with operator-variable structures.
I hypothesize, then, that feature-pair labels signal that a complex object is to be interpreted as an operator-variable structure.
For instance the Wh-question structure in \Next[a] is interpreted as in \Next[b].
\ex. \textsc{sem}($\left[_{\langle Q,Q \rangle} \textit{Who}_Q, \left[ \text{C}_Q+\textit{did}, \left[ \textit{Mary see } t_{Who} \right] \right]  \right]$) = (Wh\textit{x})(\textsc{sem}(\textit{Mary saw x}))

Finally, the case of unlabelled structures, which is identical to the case of Host-Adjunct structures.
In this case, objects compose by conjunction.
Consider the interpretation of the structure in \Next.
\ex. \textsc{sem}($\left[_\emptyset \textit{run, swiftly} \right]$) =  $(\lambda e)(\textsc{sem}(\textit{run})(e) \& \textsc{sem}(\textit{swiftly})(e))$

In many ways this proposal is a synthesis of aspects of three extant theories of semantic composition, but it is grounded in syntactic theory rather than semantic theory.
The first theory, already mentioned above, is that of \textcite{heimkratzer1998semantics}, which has two related components: lexically specified semantic types, and multiple principles of composition.
The types they assume are generated by a recursive definition given in \Next, and each lexical item has one and only one type.
\ex. \textbf{Type definition}
\a. $e$ and $t$ are types.
\b. For all $\sigma, \tau$. if $\sigma$ is a type and $\tau$ is a type, then $\langle\sigma,\tau\rangle$ is a type.
\c. Nothing else is a type.

So, for instance, a quantifier like \textit{every} is of type $\langle\langle e,t\rangle, \langle\langle e,t\rangle, t\rangle\rangle$, while a transitive verb like \textit{hit} is of type $\langle e, \langle e,t\rangle\rangle$.
<+Finish+>

\textcite{partee1987noun}
\end{document}
