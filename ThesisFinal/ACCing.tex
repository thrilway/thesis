%        File: ACCing.tex
%     Created: Tue Feb 21 02:00 PM 2017 E
% Last Change: Tue Feb 21 02:00 PM 2017 E
%
% arara: pdflatex: {options: "-draftmode"}
% arara: biber
% arara: pdflatex: {options: "-draftmode"}
% arara: pdflatex: {options: "-file-line-error-style"}
\documentclass[MilwayThesis]{subfiles}

\begin{document}
Another issue with my account has to do with the sideward movement operation.
Recall that in order to derive an adjectival resultative, a DP must move sideward from the resP adjunct to the VP as in \cref{fig:ResStruct}.
\begin{figure}[h]
	\centering
	{\small
\begin{forest}
    nice empty nodes,sn edges,baseline,
%    for tree={
%    calign=fixed edge angles,
%    calign primary angle=-30,calign secondary angle=60}
    [VP
	    [VP
		    [hammer]
		    [DP[the metal,roof,name=compV]]
	    ]
	    [resP
		    [$\langle$DP$\rangle$,name=specRes]
		    [
			    [res]
			    [SC
				    [$\langle$DP$\rangle$,name=SCDP]
				    [
					    [adj]
					    [\textsc{flat}]
				    ]
			    ]
		    ]
	    ]
    ]
    \draw[->] (SCDP) to[out=south west, in=south] (specRes);
    \draw[->] (specRes) to[out=south, in=south east] (compV);
\end{forest}
}
\repeatcaption{fig:ResStruct}{The structure of a resultative}
\end{figure}
Note also that that sideward movement operation seems to be obligatory; that is, the DP that originates in the resP must also appear as the theme of the VP.
This obligatoriness can be seen in the fact that \cref{ex:double-theme-res} is ungrammatical.
\ex.*  Sam [$_{VP}$ hammered the nail] [$_{resP}$ the planks together].\label{ex:double-theme-res}\\
($\approx$ Sam hammered the nail and, as a result, the planks were fastened together)

An easy way of accounting for this would be to hypothesize that it is due to some property of the res head, and if this obligatory sideward movement were particular to resultatives, then, indeed, this would likely be the best way to proceed.
However, sideward movement seems to be obligatory in other cases.
First, there is the case of depictives, which differ from resultatives only in the fact that they lack a res head.
Also, as I will argue in this chapter, there is  obligatory sideward movement in certain so-called ACC-ing clauses in direct perception reports such as the embedded clause in \cref{ex:ACCing1}.
\ex. We heard [them shouting at the top of their lungs].\label{ex:ACCing1}

Furthermore, I will argue that this obligatory sideward movement is, in fact, a property of adjoined phrases.
Specifically, the generalization in \cref{ex:AdjunctGen} seems to hold.
\ex.\label{ex:AdjunctGen} Internally merged specifiers of adjoined phrases must move to the host phrase.

Such a generalization, I argue in \cref{sec:paradox}, cannot be accounted for in a theory of grammar based on feature satisfaction.
Label theory, however, is able in principle to derive this generalization, but in order to do so, it must be modified and extended.
I will perform such an extension and show that the resulting modified label theory can derive \cref{ex:AdjunctGen}.

\section{On ACC-ing clauses}

\textcite{cinque1996pseudo} discusses ACC-ing clauses (ACs) under direct perception verbs as in \cref{ex:ACCingDPR} and argues that they are ambiguous, having the two structures in \cref{ex:ACCingStructs}.\footnote{
	The main object of Cinque's study, in fact, is pseudo-relatives such as those in (i), which he argues are ambiguous between the three structures in (ii).
	\ex.[(i)]\label{ex:PR}
	\a. Ho visto Mario che correva a tutta velocit\`a. (Italian)
	\b. J'ai vu Mario qui courrait \'a tout vitesse. (French)

	\ex.[(ii)]\label{ex:PRStruct}
	\a. Ho [visto [$_\text{NP}$ Mario [$_\text{CP}$ che correva \ldots ]]]
	\b. Ho [visto [$_\text{CP}$ Mario [$_{\text{C}^\prime}$ che [$_\text{IP}$ correva \ldots ]]]]
	\c. Ho [[visto Mario] [$_\text{CP}$ \textit{ec} che correva \ldots]]

	He mentions ACs briefly in order to point out that his remarks and claims about pseudo-relatives largely apply to ACs.
	The main difference between the two constructions is that ACs are not analyzable as nominals, but a related form with nominal morphology serves this function.
	\ex.[(iii)] [Their singing of the national anthem] caused an international incident.

}
\ex. I saw Mario running at full speed. \label{ex:ACCingDPR} 

\ex.\label{ex:ACCingStructs}
\a. I [saw [$_\text{ProgP}$ Mario [$_{\text{Prog}^\prime}$ -ing [$_\text{VP}$ run \ldots]]]].
\b. I [[saw Mario] [$_\text{ProgP}$ \textit{ec} running \ldots]].

I argue in \cref{sec:paradox}, the fact that a single grammar can generate both of these structures presents a serious problem for standard theories of grammar.
I will therefore discuss them in greater detail in the remainder of this section.

In one structure, represented in \cref{fig:CompACCing}, the AC is merged as the complement of the perception verb.
The interpretation of this structure is one in which the running event was seen and by virtue of the meaning of \textit{run}, seeing a running event generally entails seeing the agent of that event.
\begin{figure}[h]
	\centering
\begin{forest}
    nice empty nodes,sn edges,baseline,for tree={
    calign=fixed edge angles,
    calign primary angle=-30,calign secondary angle=70}
    [VP
	    [V\\see,align=center]
	    [ProgP
		    [DP[Mario,roof]]
		    [Prog'[running at full speed,roof]]
	    ]
    ]
\end{forest}
	\caption{Complement ACC-ing structure}
	\label{fig:CompACCing}
\end{figure}
In the second structure, represented in \cref{fig:AdjunctACCing}, the ACC-ing subject is merged as the complement of the perception verb, while the AC (with a controlled subject) is adjoined to the VP.
The interpretation of this structure is one in which \textit{Mario} is seen and the event of \textit{Mario} being seen coincides with an event of \textit{Mario} running.
Again in this interpretation, due to the meaning of \textit{run}, seeing the agent of a running event generally entails seeing the event itself.
\begin{figure}[h]
	\centering
\begin{forest}
    nice empty nodes,sn edges,baseline,for tree={
    calign=fixed edge angles,
    calign primary angle=-30,calign secondary angle=70}
    [VP
	    [VP
		    [V\\see,align=center]
		    [DP$_i$[Mario,roof]]
	    ]		    
	    [ProgP
		    [$\langle$DP$_i\rangle$]
		    [Prog'[running at full speed,roof]]
	    ]
    ]
\end{forest}
	\caption{Adjunct ACC-ing structure}
	\label{fig:AdjunctACCing}
\end{figure}
By the assumptions made here, the argument \textit{Mario} can only be shared by the verb \textit{see} and the verb \textit{run} if it is merged with both, meaning it must move from [Spec, Prog] to [Comp, V].\footnote{
	Cinque assumes that [Spec, Prog] is occupied by a controlled PRO.
}
In the case of the complement AC in \cref{fig:CompACCing}, however, the argument \textit{Mario} seems to stay in situ in [Spec, Prog], suggesting that the movement operation represented in \cref{fig:AdjunctACCing} is, in fact, optional.
If the movement operation is optional, however, we would expect two additional structures for \cref{ex:ACCingDPR}: one, represented in \cref{fig:CompACCingMove}, in which the ProgP is the complement of \textit{saw} and \textit{Mario} has moved from [Spec, Prog], and another, represented in \cref{fig:AdjunctACCingStay}, in which the ProgP is an adjunct, but \textit{Mario} does not move from [Spec, Prog].
\begin{figure}[h]
	\centering
	\begin{forest}
	    nice empty nodes,sn edges,baseline,for tree={
	    calign=fixed edge angles,
	    calign primary angle=-30,calign secondary angle=70}
	    [AgrOP
		    [DP$_i$[Mario,roof]]
		    [
			    [AgrO]
			    [VP
				    [V\\see,align=center]
				    [ProgP
					    [$\langle\text{DP}_i\rangle$]
					    [Prog'[running at full speed,roof]]
				    ]
			    ]
		    ]
	    ]		
	\end{forest}
	\caption{Complement ACC-ing structure with object raising}
	\label{fig:CompACCingMove}
\end{figure}
\begin{figure}[h]
	\centering
	\begin{forest}
	    nice empty nodes,sn edges,baseline,for tree={
	    calign=fixed edge angles,
	    calign primary angle=-30,calign secondary angle=70}
	    [VP
		    [VP
			    [V\\see,align=center]
		    ]		    
		    [ProgP
			    [DP$_i$[Mario,roof]]
			    [Prog'[running at full speed,roof]]
		    ]
	    ]
	\end{forest}
	\caption{Adjunct ACC-ing structure without DP movement}
	\label{fig:AdjunctACCingStay}
\end{figure}
If we consider the consequences of this proposed optionality hypothesis, we can see that it cannot be true.

First, consider the structure in \cref{fig:AdjunctACCingStay}, in particular the fact that \textit{see} does not have an internal argument.
This is not \textit{per se} problematic, as verbs may be optionally transitive, but we would expect that \textit{see} in this structure could have an internal argument other than \textit{Mario}.
That is, if \cref{fig:AdjunctACCingStay} is a possible structure for \cref{ex:ACCingDPR}, then we would expect \cref{ex:ACCingDouble} to also be a licit sentence.
\ex.* I [$_\text{VP}$ [$_\text{VP}$ saw Sue] [$_\text{ProgP}$ Mario running at full speed]]. \label{ex:ACCingDouble}

If \textit{Mario} can remain in [Spec, Prog], then we have no way to rule out \textit{Sue} merging with \textit{see} and behaving as a direct object.
Of course, \cref{ex:ACCingDouble} is ungrammatical, suggesting that \textit{Mario} cannot remain in [Spec, Prog] if ProgP is adjoined to VP.
Movement from [Spec, Prog], then, cannot be optional, strictly speaking.
If it is not optional, perhaps it is obligatory.

If movement from [Spec, Prog] is obligatory, then we must revise Cinque's analysis of complement ACs.
Suppose, then, that \textit{Mario}, in the complement ACC-ing analysis of \cref{ex:ACCingDPR}, must raise to object.
In other words, suppose \cref{fig:CompACCingMove} is a possible structure of \cref{ex:ACCingDPR} and \cref{fig:CompACCing} is not.
Note that in \cref{fig:CompACCingMove}, \textit{Mario} is the grammatical object but not the theme of \textit{see}.
If this is the case then we expect that \textit{Mario} can become the subject of a passive derived from \cref{fig:CompACCingMove}.

Indeed, subjects of ACs can become passive subjects as in \cref{ex:ACCingPassive}, but it is not immediately obvious whether \cref{ex:ACCingPassive} is derived from an Adjunct AC structure or a Complement AC structure.
\ex. Mario was seen running at full speed. \label{ex:ACCingPassive}

If \cref{ex:ACCingPassive} had been derived from a Complement ACC-ing structure, however, then \textit{Mario} would not have been $\theta$-marked by \textit{see}.
So, in order to test whether passives like \cref{ex:ACCingPassive} can be generated in which the subject is not interpreted as the theme of the verb, that is, we need a clause of the form in \cref{ex:ACCingPassiveTempl} where the event of V-ing was perceived without the individual X being perceived.
\ex. X was $\left\{ \text{seen/heard/felt} \right\}$ V-ing \ldots\label{ex:ACCingPassiveTempl}

There are certain classes of predicates which we can use as diagnostics due to a non-canonical event/argument structure.
Consider the ACC-ing versions of weather reports and clausal idioms, for instance, as given in \cref{ex:ACCingWeather} and \cref{ex:ACCingIdiom}
\ex. Bill saw it snowing. $\centernot\implies$ Bill saw it. \label{ex:ACCingWeather}

\ex. Bill heard all hell breaking loose. $\centernot\implies$ Bill heard all hell. \label{ex:ACCingIdiom}

Consider, also, the predicates \textit{be slandered} and \textit{be parodied}.
Events of parodying or slandering an individual $x$ generally do not include $x$ as a participant the way, for instance, events of hitting $x$ or speaking to \textit{x} do.
This is demonstrated in \cref{ex:ACCingSlander} and \cref{ex:ACCingParody}
\ex. We heard the writer being slandered. $\centernot\implies$ We heard the writer. \label{ex:ACCingSlander}

\ex. They saw the singer being parodied. $\centernot\implies$ They saw the singer. \label{ex:ACCingParody}

Unlike most direct perception reports with ACs, then, \crefrange{ex:ACCingWeather}{ex:ACCingParody} are not ambiguous.
Rather, they have only the complement AC structures.
Therefore, if the perception reports in \crefrange{ex:ACCingWeather}{ex:ACCingParody} can be passivized, this will be evidence for the proposal that DPs are able to move out of complement ACs.
In fact, it seems that they cannot be passivized.
\ex.* It was seen raining.

\ex.* All hell was heard breaking loose.

\ex.* The writer was heard being slandered.\label{ex:SlanderedPassive}

\ex.* The singer was seen being parodied.\label{ex:ParodiedPassive}

I can think of no principled explanation of these facts except to propose that DP movement out of a complement AC is barred.
Absent any evidence or argument to the contrary, then, I will assume that Cinque's initial analysis of complement ACs was correct.

Thus we are led to the following generalization with respect to ACs:
If an AC is adjoined to a VP, then its subject must move, but if an AC is merged as the complement of a verb, then its subject cannot move.
This is an unexpected, perhaps unprecedented syntactic generalization.
In fact, I will argue in the following section that such a pattern is predicted to be impossible by any theory that defines grammaticality solely in terms of feature satisfaction, as standard minimalist theories do.
\section{Feature satisfaction cannot account for ACC-ing clauses}\label{sec:paradox}
As I discussed in \cref{sec:nonstandard}, standard minimalist theories tend to assume that a derived syntactic object converges at the interfaces iff it contains no unsatisfied features.
There is, of course, debate as to what it means for a feature to be unsatisfied, and what sort of operations are able to satisfy these features.
Therefore, I will attempt to abstract away from the details of particular theories and discuss what I take to be their shared assumptions.

The first common assumption (or set of assumptions) is about features.
Lexical items bear or consist of features, each of which is inherently either satisfied or unsatisfied,\footnote{
	The two most common versions of unsatisfied features and satisfaction operations are \textit{uninterpretable} features which must be checked, and \textit{unvalued} features which must be valued.
}
 and each of which is also specified for the information it encodes (person, gender, tense, etc.).
For instance, a determiner may bear a satisfied definiteness feature and an unsatisfied Case feature.

The second common assumption is that some computational operation converts a token of some unsatisfied feature into a token of the corresponting satisfied feature under the influence of some other feature token.
In order for feature token F on lexical item token X to satisfy feature token G on lexical item token Y, X and Y must stand in some structural relation to each other, and F and G must be of the same type.
Furthermore, feature satisfaction is automatic; if the conditions are met for F to satisfy G, then F satisfies G.

The third common assumption, is that there is no operation which undoes feature satisfaction.
This is never made explicit, but it is nonetheless assumed to be true.
Such an assumption, in effect, ensures a certain monotonicity in syntactic derivations, in that if at some derivational stage S$_n$ feature token F is satisfied, then there is no later stage S$_{n+i} (i > 0)$ at which F is unsatisfied.

The fourth common assumption, which I have already alluded to, is that a syntactic object is well-formed iff it contains no lexical items with any unsatisfied features.
This will be used as a diagnostic for satisfied features.
If a sentence, phrase, or word is well-formed, then it must contain no unsatisfied features, and if an expression is ill-formed, then it must contain unsatisfied features.

With these assumptions in place, consider the complement AC case in \cref{ex:CompACCing}, which is well-formed; this means that all of its sub-parts are well-formed.
\ex. They [$_\text{VP}$ saw [$_\text{ProgP}$ the raccoon [walking across the street]]].\label{ex:CompACCing}

Since the ProgP is well-formed, it must contain no unsatisfied features, and, therefore, the DP \textit{the raccoon} must contain no unsatisfied features.
Assuming that \textit{the raccoon} is in [Spec, Prog], rather than its base position in [Spec, $v$/Voice], then at least one of its unsatisfied features can only be satisfied in [Spec, Prog]. 
\ex.
\a.* They [$_\text{VP}$ saw [$_\text{ProgP}$ walking [$_\text{DP}$ the raccoon] across the street]].
\b.* They [$_\text{VP}$ saw [$_\text{ProgP}$ [-ing [[$_\text{DP}$ the raccoon] walk across the street]]]].

In other words, \textit{the raccoon} is licensed in [Spec, Prog].

However, when we consider the adjunct AC in \cref{ex:AdjuACCing}, we come very quickly to a contradiction.
\ex. They [$_\text{VP}$ [$_\text{VP}$ saw the raccoon][$_\text{ProgP}$ $t$ walking across the street]].\label{ex:AdjuACCing}

The fact that a lower copy/trace occupies [Spec, Prog] indicates that \textit{the raccoon} is not licensed there.
In fact, the data adduced above in \cref{ex:ACCingDouble} and the discussion thereof indicates this fact even more forcefully.
If \textit{the raccoon} is not licensed in [Spec, Prog], this means it bears some feature which cannot be satisfied there.
This is a direct contradiction of the conclusion I came to above, and yet this contradiction arises from an analysis of facts based on an axiom set.
The conclusion I draw from this contradiction is that the axiom set (a.k.a. the feature-satisfaction theory of syntax) is fundamentally flawed.

In order to pinpoint this flaw, we should consider how the facts could be generalized.
I believe a proper expression of the generalization is given in \cref{ex:AdjunctACCingGen} and \cref{ex:CompACCingGen}.
\ex. A syntactic object X (= $\left\{ t, \left\{ \text{Prog, YP}  \right\} \right\}$) is well-formed only if X is an adjunct.\label{ex:AdjunctACCingGen}

\ex. A syntactic object X (= $\left\{ \text{DP}, \left\{ \text{Prog, YP}  \right\} \right\}$) is well-formed only if X is an argument.\label{ex:CompACCingGen}

This is a problem for the feature-satisfaction theory because implicit in the theory is the claim that the well-formedness of an object depends solely on its internal structure.
The generalizations here, however, make reference, not just to the internal structure of an object, but also to the larger structure that the object is a part of.
This focus on internal structure is to be expected if the grammaticality of an expression is solely determined by whether the narrow syntax operates properly.
That is, if a given operation in the NS must be justified locally, it follows that it cannot be justified on the basis of an operation which has yet to occur.

However, A theory which bases grammaticality, at least partially, on interface conditions, as label theory does, will be able to account for the generalizations in \cref{ex:AdjunctACCingGen} and \cref{ex:CompACCingGen}.
Specifically, label theory must treat adjuncts and arguments differently and therefore, provides a good candidate for an explanatory theory of the generalization in question.
Since host-adjunct structures are always a species of phrase-phrase structures,\footnote{
	The phrasal nature of clausal and PP adjuncts for instance is uncontroversial, but adjectives and adverbs are also phrasal given the theory of categories assumed here.
	An adverb, for instance, consists minimally of a root and an $adv$ head.
}
we would expect a labelling paradox as with other phrase-phrase structures.
Unlike other phrase-phrase structures, however, there does not seem to be an agreement-based ``repair strategy'' for host-adjunct structures.
Almost by definition, adjuncts neither move, nor agree. 
Adjuncts, it seems, simply do not enter into the labelling calculation.
However, label theory as formulated in \cite{chomsky2013problems} and \cite{chomsky2015problems} does not address host-adjunct structures at all, and therefore is not quite suitable for my purposes.
Thus I will modify it in the following section.

\section{Modifications to label theory}\label{sec:modifications}

In this section I will address two questions that \textcite{chomsky2013problems,chomsky2015problems} largely leaves open.
First, there is the question of how to label Host-Adjunct structures, which I address in \cref{sec:adjuncts}.
Second, There is the question of why labels are required at all.
I address this question in \cref{sec:label-sem}.
%\subfile{Modifications}
\subsection{Labelling Host-Adjunct Structures}\label{sec:adjuncts}
To understand how adjuncts behave with respect to labelling, let's consider their basic properties: optionality, iterativity, and freedom of order.
These can be demonstrated in the series of sentences in \Next.
\ex. 
\a. The protester was brought to the police station.
\b. The protester was brought to the police station, against her will.
\b. The protester was brought to the police station, against her will, after the demonstration.
\b. The protester was brought to the police station after the demonstration, against her will.
\z.

If we assume that the adjuncts in \Last are adjoined to TP, then the TPs in each of the sentences in \Last, are, in some sense, grammatically indistinguishable.
If we take this much for granted, then we can view the task of developing a theory of adjuncts to be the task of making explicit the sense in which the sentences in \Last are indistinguishable.
Assuming label theory, we can make a fairly trivial explication of the indistinguishability of these sentences: the TPs in \Last are indistinguishable in the sense that they are labelled identically.
If the sentences in \Last are constructed purely by Merge (and Select, and Copy), then they have the structures in \Next.
\ex.
\a. [$_\alpha$ The protester was brought to the police station].
\b. [$_{\beta} [_{\alpha}$The protester was brought to the police station], [against her will]].
\b. $[_{\gamma}[_{\beta}[_{\alpha}$The protester was brought to the police station], [against her will]], [following the demonstration]].
\b. $[_{\eta} [_{\delta} [_{\alpha}$The protester was brought to the police station] [following the demonstration]], [against her will]].
\z.

If we take $\alpha$ to be the TP without adjuncts, then its label will be the basis for the label of the modified TPs $\beta, \gamma, \delta$ and $\eta$.
Since $\alpha$ is a finite TP with a subject, its label will be $\langle\varphi,\varphi\rangle$, and by assumption, the label of each of the modified TPs will be $\langle\varphi,\varphi\rangle$.
If this is the case, then the adjoined phrases contribute nothing to the labelling algorithm; in other words, they are invisible to LA.
The invisibility of adjuncts cannot, however, be the same phenomenon as the invisibility of lower copies, as the latter arises from a movement operation, and there is no reason to think that adjunct phrases as a class undergo movement.
Furthermore, even if adjuncts did move, this would only explain why lower copies are invisible; we would still need to explain why higher copies are invisible.

If the invisibility of adjuncts cannot be derived syntactically, perhaps it is inherent.
That is, perhaps the set of adjuncts is a natural class of objects which are invisible to LA.
This suggestion, however, runs into problems almost immediately due to the fact that there are phrases which can be arguments, predicates, or adjuncts as in \crefrange{ex:Adjective}{ex:PP}.
\ex. \label{ex:Adjective}
\a. The green room. (\textit{green} as an adjunct)
\b. The room is green. (\textit{green} as a predicate)

\ex. \label{ex:PP} 
\a. Meryl swam [in the pool]. (PP adjunct)
\b. Cameron fell [in the pool]. (PP complement)

It seems that, without a significant amount of stipulation, this is not a promising approach, so I will not pursue it further.

Adjunction, then, cannot be reduced to simplest Merge.
This leaves two broad options for assimilating it into our theory.
The first is to propose a new operation in Narrow Syntax (NS) that generates adjunction structures.
\textcite{chomsky2004beyond} proposes an operation of pair-Merge, which, given a host object $\beta$ and an adjunct $\alpha$, creates the object $\langle\alpha,\beta\rangle$ ($\alpha$ adjoined to $\beta$).
The new object $\langle\alpha,\beta\rangle$, however, has all of the syntactic properties (c-command relations, $\theta$-roles, selectional properties, etc) of the previously generated object $\beta$.
So, as far as NS is concerned, $\langle\alpha,\beta\rangle$ is equivalent to $\beta$.
For preciseness, I will use the term $\sigma$\textit{-equivalent}: an object $\langle\alpha,\beta\rangle$ (created by pair-Merge) is $\sigma$-equivalent to $\beta$.
There are two problems with this proposal, which I address in turn in the following paragraphs.

The first problem with adding an operation of pair-Merge to the system arises from the question of whether that change violates SMT.
Recall that SMT states that the language faculty is an optimal solution to interface problems, meaning that a minimalist theory of grammar should only admit complications if they are required due to interface conditions.
So, is pair-Merge required by one of the interfaces?
The fact that the information expressed by pair-Merge can be expressed periphrastically, as shown in \cref{ex:Periphrasis} suggests that pair-Merge is extraneous.
\ex.\label{ex:Periphrasis}
\a. I'd like a large burger with ketchup.\label{ex:succinct}
\b. I'd like a burger. I'd like it to be large. I'd like it to have ketchup.\label{ex:verbose}

The series of sentences in \cref{ex:verbose} express the same proposition as the single sentence in \cref{ex:succinct} and they would do so without any instances of pair-Merge.
The same cannot be said about structures formed by set-Merge; an expression constructed by set-Merge cannot be paraphrased without set-Merge.
Since it is not required by the interfaces, the addition of pair-Merge to NS would constitute a violation of SMT.

The second problem with pair-Merge arises from concerns about economy of derivation.
There are two facts about pair-Merge that are relevant to this issue.
First, pair-Merge is a more complex operation than set-Merge, as the former induces order, while the latter does not.
And second, when we adjoin an object to a host by pair-Merge, the resulting object is $\sigma$-equivalent to the host without the adjoined object, in the sense that a noun phrase with an adjective adjoined to it has all of the same syntactic properties as that same noun phrase without any adjunct.
Consider the two sub-derivations in \cref{ex:pmerge-deriv} and \cref{ex:smerge-deriv}.
The results of the two derivations are $\sigma$-equivalent to each other, but the first dervation is more complex than the second one.
\ex.\label{ex:pmerge-deriv} pMerge(X, Y) = $\langle\text{X, Y}\rangle$\\
Merge(Z$, \langle\text{X, Y}\rangle$) = $\left\{ \text{Z}, \langle\text{X, Y}\rangle \right\}$

\ex.\label{ex:smerge-deriv} Merge(Z, Y) = $\left\{ \text{Z, Y} \right\}$

From the view of NS, then, pair-Merge does a lot of work to no effect; the object derived in \cref{ex:pmerge-deriv} with an adjunct is syntactically indistinguishable from the object derived in \cref{ex:smerge-deriv} without an adjunct.
Therefore, for every derivation D that uses pair-Merge there is a simpler derivation D$^\prime$ such that the result of D is $\sigma$-equivalent to that of D$^\prime$.
This is exactly the type of situation that derivational economy rules out.

So, if, as I argue above, adjunction does not occur in NS, then it must occur after NS---the secon broad option for incorporating adjunction into our theory.
However, if we make the standard minimalist assumption that NS is the only module capable of recursively combining expressions to form larger expressions, then there can be no recursive combinatory operation outside of NS.
Therefore, adjunction -- being outside of NS -- cannot be a recursive combinatory operation. 
That is, adjunction does not create new syntactic objects.
This means that our way of representing adjunction in tree structures is misleading.

Consider, for instance, the modified VoiceP in \cref{ex:WithGusto} as represented in \cref{fig:WithGusto}.
\ex. Mary sang the song with gusto.\label{ex:WithGusto}

\begin{figure}[h]
	\centering
	\begin{forest}
	    nice empty nodes,sn edges,baseline,
%	    for tree={
%	    calign=fixed edge angles,
%	    calign primary angle=-30,calign secondary angle=70}
	    [$\beta$
		    [$\alpha$
			    [DP[Mary,roof]]
			    [$\gamma$
				    [Voice]
				    [VP[sing the song,roof]]
			    ]
		    ]
		    [PP[with gusto,roof]]
	    ]
	\end{forest}
	\caption{A standard representation of a modified VoiceP}
	\label{fig:WithGusto}
\end{figure}
The object $\beta$ is usually taken to be created by adjoining the PP \textit{with gusto} to $\alpha$, but, as I argued above, adjunction cannot create new objects.
Therefore, there is no object $\beta$.
This is, no doubt, a surprising conclusion, yet it follows from the basic facts of adjunction and SMT, so it behooves us to entertain it as a possibility.

This conclusion, in fact, does resolve the immediate question of how Host-Adjunct structures are labelled, not by answering it but by dissolving it.
If LA is a function from unlabelled SOs to labelled SOs and Host-Adjunct structures are not SOs, then they are outside the domain of LA.
However, there are a few caveats that bear mentioning.
The first is that, while Host-Adjunct structures are not properly SOs, the same cannot be said for the adjuncts \textit{per se}.
So, to consider a concrete case, $\beta$ in \cref{fig:WithGusto} is not an SO, but the PP \textit{with gusto} is an SO, meaning it was derived in NS and labelled by LA.
The second caveat is that not everything that looks like a Host-Adjunct structure is one.
For instance the topicalized PP in \cref{ex:WithSorrow} is likely an argument of, say, a functional projection Topic.
\ex.\label{ex:WithSorrow} With sorrow in her heart, Mary sang the song.

The third caveat, which perhaps is more of a promissory note, is that asserting that Host-Adjunct structures are not SOs leaves us with the question of what they are.
This is far from an easy question to answer, and I will not attempt a complete answer here.
Instead, I will stipulate that, at the CI interface, a host-adjunct structure is a complex object which is asymmetric and unlabelled.
It is asymmetric in the sense that the host is more prominent than its adjuncts.
A proper theory of adjunction, if one exists, will derive this asymmetry from intrinsic properties of the host and adjuncts, but I will stipulate it here.
It is unlabelled because only SOs are labelled, and host-adjunct structures are not SOs.

The notion that there can be complex linguistic objects which are not generated by Merge may seem to contradict the evolutionary version of SMT, which states that the evolution of the language faculty consists in the sudden appearence of Merge.
If adjunction is a non-Merge method for constructing complex linguistic expressions, then we would expect there to be a language faculty even without Merge.
While this expectation is not, strictly speaking, borne out, there does seem to be an extra-linguistic cognitive system that makes use of complex language-like representations.
Consider the system of propositional attitudes that \textcite{fodor1975language} discusses, and the structures employed in the study of discourse pragmatics.

For Fodor, all cognition involves several sets of propositions that a given organism has certain attitudes toward.
For instance, every animal has a set of beliefs and a set of desires, which are populated by propositions.
The fact that humans have language means that these propositions can be of arbitrary complexity, but they are still beliefs and desires.
Also, perhaps the most central notion of discourse pragmatics is the common ground, which is a set of propositions believed to be shared between discourse participants.
While the common ground interacts with linguistic expressions, it does not seem to be one itself.
These examples are complex cognitive objects that are non-linguistic, and, like host-adjunct structures, they ``compose'' by conjunction.

While these proposition sets are not usually considered to be compositional, it seems rather obvious that, holding an attitude towards a set of propositions is logically equivalent to holding that same attitude towards the conjunction of the proposition in the set.
\ex. \textsc{bel}(p) \& \textsc{bel}(q) $\leftrightarrow$ \textsc{bel}(p \& q)

Note, of course, that this is a logical equivalence but not a representational equivalence,\footnote{
	\citeauthor{fodor1975language}'s (\citeyear[50--100]{fodor2010lot}) discussion of referential opacity, provides, I believe, an excellent argument for distinguishing logical equivalence from representational equivalence.
} and since, according to the computational theory of mind, representations matter, I would not like to claim that holding an attitude towards a set of propositions P necessarily requires holding that attitude towards the conjunction of every subset of P.
Rather, I claim that if two propositions, p and q, are members of the same set in the mind, then the operation of adding the conjunction of those propositions, p\&q is available, but an operation of adding other possible compositions of p and q (p$\vee$q, p$\rightarrow$q, etc.) is not available.
So, perhaps the process of interpreting a host-adjunct structure involves adding the conjunction it expresses to some proposition set.

To summarize, the above discussion was a long-winded way of hypothesizing that host-adjunct structures not only are not labelled by LA, but are not processed by LA.

\subsection{Why are labels needed at all?}\label{sec:label-sem}
The second question is why labels should be required by the CI interface at all.
My proposed answer is that the label of a complex object determines how that object composes semantically.
While this may seem \textit{ad hoc}, it is actually a fairly reasonable hypothesis.
Consider Chomsky's labelling hypothesis as phrased in \Next, and the more standard theory of the CI interface in \NNext.
\ex. A syntactic object is a valid CI object iff it is labellable.

\ex. A syntactic object is a valid CI object iff it composes semantically.

At first glance, these hypotheses are incompatible, giving us three options for resolving the conflict.
The first option would be to reject one of the conflicting hypotheses.
There is no strong evidence, however, for rejecting either \LLast or \Last, so I will not choose this option.
The second option is to conjoin the iff clauses as in \Next.
\ex. A syntactic object is a valid CI object iff it is labellable and it composes semantically.

This option is unattractive for reasons of theoretical parsimony, so I will not choose it.
The third option is to hypothesize that labelling and composition are two sides of the same coin, and therefore the conflicting hypotheses are equivalent.
We can, then, replace our two conflicting statements with the two compatible statements in \Next and \NNext below.
\ex. A syntactic object composes iff it is labellable.

\ex. A syntactic object is a valid CI object iff it composes.

This move is theoretically attractive partially due to the fact that it mirrors the logic of antisymmetry on the SM interface \parencite{kayne1994antisymmetry}.
In the case of antisymmetry, Kayne identifies asymmetric c-command with linear order, and there is no compelling reason to think that the CI interface should be more complex than the SM interface.

So, what would it mean for labelling and composition to be two sides of the same coin?
Again, it is helpful to consider the SM interface, where asymmetric c-command and linear order are associated because they are isomorphic.
We should expect a similar isomorphism to hold between composition and labels, and, in fact, there seems to be good reason to think that there is such an isomorphism.
Consider the main modes of composition generally assumed by semanticists \parencite[\textit{e.g.}, by][]{heimkratzer1998semantics}, given schematically in \Next.
\ex. 
\a. \textbf{Lexical insertion}\\
\textsc{sem}($\alpha$) = $\alpha^\prime$
\b. \textbf{Function application}\\
\textsc{sem}($\left[ \alpha, \beta \right]$) = \textsc{sem}($\alpha$)(\textsc{sem}($\beta$))
\b. \textbf{Predicate modification}\\
\textsc{sem}($\left[ \alpha, \beta \right]$) = \textsc{sem}$(\alpha)(x) \&$ \textsc{sem}$(\beta)(x)$
\b. \textbf{Predicate abstraction}\\
\textsc{sem}($\left[ \alpha, \beta \right]$) = (Op$x$)(\textsc{sem}($\beta$)($x$))

Each of these modes of composition has a corresponding structure type as identified by the version of label theory developed here.
Abstracting away from phrasal idioms, lexical insertion operates on a single syntactic atom, \textit{i.e.}, a head, which label theory necessarily distinguishes from other syntactic objects.
Predicate modification is the next most complex operation: it conjoins two (possibly complex) objects without requiring or inducing any ordering of the two, exactly isomorphic with the output of merge: unlabelled and unordered syntactic objects.
Function application, likewise, requires two objects, but these objects are ordered.
Unlike conjunction structures created by predicate modification, which are commutative ($X \& Y = Y \& X$), the function-argument structures created by function application are inherently asymmetric ($X(Y) \neq Y(X)$).
This matches with head-labelled structures, which encode a pair of objects (the contents of the structure) and an ordering statement (the label).
Finally, predicate abstraction, which creates structures similar to quantifier structures, requires the content of the two expressions, an ordering between the two, and a variable.
Pair-labelled structures provide this information.

First consider those structures labelled by heads.
The classes of structures which get head labels are given in \Next.
\ex. \textbf{Head-labelled structures}
\a. $\left\{ \text{X, }\textsc{Root} \right\} \xrightarrow{Label} \left[_\text{X} \text{X, }\textsc{root}  \right]$
\b. $\left\{ \text{X, YP} \right\} \xrightarrow{Label} \left[_\text{X} \text{X, YP} \right]$
\c. $\left\{ t_\text{ZP}, \left\{ \text{X, YP} \right\} \right\}\xrightarrow{Label}\left[_\text{X} t_\text{ZP}, \text{XP} \right]$

I propose that in these cases, the objects compose by function application, with the label being the function and the non-labelling constituent being the argument.
So, for instance, a DP is interpreted as the function D, with NP as an argument.
\ex. \textsc{sem}($\left[_\textit{the} \textit{the, ball} \right]$) = \textsc{sem}(\textit{the})(\textsc{sem}(\textit{ball}))

Next, consider the structures labelled by feature-pairs.
These structures tend to be the result of internal Merge, which is generally associated with operator-variable structures.\footnote{
	I use the hedges \textit{tend} and \textit{generally} here to indicate that I was not able to perform an exhaustive enumeration of all pair-labelled structures.
	I perhaps could have formulated this generalization without hedges, in which case this footnote would be explaining that perhaps I should have hedged the generalization slightly.
}
I hypothesize, then, that feature-pair labels signal that a complex object is to be interpreted as an operator-variable structure.
For instance the Wh-question structure in \Next[a] is interpreted as in \Next[b].
\ex. \textsc{sem}($\left[_{\langle Q,Q \rangle} \textit{Who}_Q, \left[ \text{C}_Q+\textit{did}, \left[ \textit{Mary see } t_{Who} \right] \right]  \right]$) = (Wh\textit{x})(\textsc{sem}(\textit{Mary saw x}))

Finally, we come to the case of unlabelled structures, which is identical to the case of Host-Adjunct structures.
As I discussed in \cref{sec:adjuncts}, however, Host-Adjunct structures are unlabelled because they are not SOs and the domain of the Labelling Algorithm is restricted to SOs.
These structures are given the default interpretation of conjunction, as discussed in \cref{sec:adjuncts}.

I have identified Kayne's (\citeyear{kayne1994antisymmetry}) theory of the SM interface as an inspiration for my proposal, and I would like to say a little more about the similarities between his and my hypotheses.
Rather than positing an active process, be it simple or intricate, for linearizing a hierarchical structure, Kayne suggests that linear order is the product of a passive isomorphism.
Since asymmetric c-command \textit{is} a linear, or total, order, a hierarchical structure can be mapped to a linear string based purely on properties of that structure.
The idea that an interface between mental modules should be passive is in keeping with the very idea of modularity.
If the SM module and the Narrow Syntax module are truly independent, then we would not expect there to be any specialization of one in order to interact with the other.
My proposal for the CI interface is, I believe, a step towards a passive interface.
Although evidence for the nature of the CI module is not as readily available as evidence for the nature of the SM module, the working, albeit tacit, assumption seems to be that the CI module deals in representations that are formally very similar to formulas of predicate logics.
If we assume a predicate logic with operators\footnote{Since the term \textit{operator} already has a particular meaning in generative syntax, I will refer to the operators of predicate logic as \textit{l-operators}.} ($\forall$, $\exists$, $Wh$, M,\ldots), functions/predicates ($P, Q, f, g,$\ldots), and variables ($x, y, z,$\ldots), then we can see how there could be an isomorphism between labelled syntactic objects and formulas of this logic.

While it seems that SM objects are linear structures (strings), CI objects seem to be more complex, including notions of scope and variable binding.
I therefore cannot able to give a simple order-theoretic explanation of the CI interface as \textcite{kayne1994antisymmetry} gives for the SM interface.
What I can do, however, is give a coarse-grained mapping between labelled SOs and expressions in predicate logic.
The class of labelled SOs and that of complex expressions of predicate logic can each be divided into two sub-classes.
Labelled SOs can be head-labelled, or pair-labelled, while expressions of predicate logic can be function-argument expressions, or l-operator expressions.
Furthermore, these sub-classes seem to map to each other as shown in \Next.
\ex.
\begin{tabular}[t]{ll}
	Labelled SO & Predicate logic\\
	\hline
	\hline
	Head labelled & Function-argument\\
	$\left\{ \text{in}, \left\{ \text{in, the snow} \right\} \right\}$ & $\textbf{in}(\textbf{the\_snow})$\\
	\hline
	Pair-labelled & L-operator expressions\\
	$\left\{ \langle Q,Q\rangle, \left\{\text{Who}_Q, \left\{\text{C}_Q, \text{fell} \right\}  \right\} \right\}$ & $\text{Wh}x(\textbf{fell}(x))$\\
	\hline
\end{tabular}

This mapping is, of course, a first approximation of a theory of the CI interface.
Being a first approximation, it will face empirical and theoretical challenges.
For instance, there are likely to be cases where an expression's label does not seem to map to its interpretation.
These cases, however, might be cases in which our syntactic or semantic analysis is incorrect.

Furthermore, even if the hypothesized mapping in \Last were shown to be empirically adequate, it would still require theoretical explanation.
That is, we would need to explain why that particular mapping holds.
This would require us to show that there is a mathematically sound isomorphism between head-labelled SOs and function-argument expressions, and between pair-labelled SOs and l-operator expressions.
Such a demonstration, however, is beyond the scope of this thesis.
\section{Explaining ACC-ing clause subjects}\label{sec:ExplainingACCingSubjs}
With these modifications of label theory in place, we can consider how to account for the distribution of ACC-ing clause (AC) subjects.
First, I will explain the fact that, in the case of complement ACs, the subject DP cannot move.
This explanation will be essentially the same as the explanation of ECP effects given in \cite{chomsky2015problems}.
I will then explain the adjunct AC case, in which the subject DP must move.
This explanation will require the modified version of label theory and will support the general restriction in \cref{ex:AdjunctGen}.

As in Chomsky's (\citeyear{chomsky2015problems}) analysis of \textit{that}-trace effects and my explanation of the non-generation of resultatives in French-like languages in \cref{sec:Fre-deriv}, a DP can be blocked from moving out of a phrase $\left\{ \text{DP, XP} \right\}$ if the following conditions hold.
First, the head X of XP bears features which must be valued by a DP in order to label XP.
That is, X must bear an incomplete set of, say, $\varphi$-features.
Second, if the DP moved, then that movement would occur before DP and XP agree for the features in question.
In other words, if a movement operation bleeds agreement and labelling requires that agreement relation, then that movement operation will be ruled out.
Applying this logic to the complement AC, I hypothesize that the English Prog head has an incomplete $\varphi$-set which must be strengthened by Agree in order to provide a label.
If this is the case, then we can explain the impossibility of movement from complement ACs if such a movement would bleed agreement.

The case of adjunct ACs, however, requires a more complicated explanation.
The explanation will be split into two subsections: \Cref{sec:DPCanMove} will show that a DP can move out of an adjunct AC without causing a problem, and \cref{sec:DPMustMove} will argue that an in-situ DP in an adjunct AC runs into problems.

\subsection{DPs can move from adjunct ACC-ing clauses}\label{sec:DPCanMove}
In order to show that movement from an adjunct AC is permitted, I will demonstrate, in some detail, how an adjunct AC structure is derived and interpreted.
First, let's consider how the AC in \cref{ex:JoannaTeaching} is derived.
\ex.\label{ex:JoannaTeaching} Joanna teaching her students.

Since the ``contentful'' portion of the AC---the portion that encodes lexical information and thematic structure---is almost entirely independent of the larger AC structure, I will assume its derivation is uncomplicated.
That is, I will take for granted that, if the complement of Prog is a VoiceP, and that that VoiceP is derived and labelled as it would be in a grammatical finite clause.
Further support for this assumption comes from \textcite{harwood2015being} who argues that Prog is a phase head, and therefore its introduction triggers the transfer of its complement, although not before the ACC-ing subject raises and merges with ProgP.
So, the AC which is to be adjoined to the host VP has the structure in \cref{fig:ProgPStruct}
\begin{figure}[h]
	\centering
	\begin{forest}
		nice empty nodes,sn edges,baseline
		[$\beta$
			[DP$_{i}$[Joanna,roof]]
			[$\alpha$
				[Prog]
				[VoiceP
					[$t_{i}$ teach her students,roof]
				]
			]
		]
	\end{forest}
	\caption{The structure of a ProgP}
	\label{fig:ProgPStruct}
\end{figure}
The next stages of the derivation requires the DP \textit{Joanna} to move out of the AC, leaving us with an unlabellable structure as per the discussion immediately preceding this subsection.\footnote{
	Prog bears a single $\varphi$-feature set, and cannot label unless that feature set is enriched by agreeing with a DP in its specifier.
	Since the DP moves before agreement can happen, Prog's feature set is not strengthened and cannot label.
}
However, a derivation doesn't necessarily crash if it's derived structure is unlabellable.
Rather it crashes when LA tries to label an unlabellable structure.
Since the AC in \cref{fig:ProgPStruct} is adjoined to VP in this case, and, by hypothesis, the labelling algorithm operates only on the host of a host-adjunct structure, the AC is not processed by the labelling algorithm.
If the AC is not processed by the labelling algorithm, then it doesn't matter whether the AC is labellable or unlabellable.
The content of an adjunct does not matter in determining the labelling of the host.

\subsection{DPs must move from adjunct ACC-ing clauses}\label{sec:DPMustMove}
Thus far, I have argued that the labellability of an adjunct is immaterial, which suggests that DP movement from an adjunct AC would be optional.
The facts of ACs, however, suggest that DPs must move from adjunct ACs.
In order to explain why the movement operation in question is obligatory, we must consider how adjunct ACs are interpreted.
Consider the illicit adjunct AC structure in \cref{fig:IllicitAdjunctAC} corresponding to the ungrammatical string in \cref{ex:IllicitAdjunctAC}.
\ex.* We can Mario the woman teaching her students.\label{ex:IllicitAdjunctAC}

\begin{figure}[h]
	\centering
\[\sbox0{$\begin{array}[]{ccc}
		\begin{forest}
	    nice empty nodes,
	    sn edges,baseline,
	    for tree={
	    calign=fixed edge angles,
	    calign primary angle=-30,calign secondary angle=70}
		    [$\delta$
			    [DP$_i$[the woman,roof]]
			    [$\gamma$
				    [Prog]
				    [VoiceP[$t_{i}$ teach her students,roof]]
			    ]
		    ] 
	\end{forest}			
	&
	\tikz[baseline=10ex,scale=1] \node[inner sep=0] at (0,-1) {\large,\,};
	&
	\begin{forest}
	    nice empty nodes,
	    sn edges,baseline,
		for tree={
	    calign=fixed edge angles,
	    calign primary angle=-30,calign secondary angle=70}
		    [$\beta$
			    [$\alpha$
				    [$v$]
				    [$\sqrt{see}$]
			    ]
			    [DP[Mario,roof]]
		    ]
	    \end{forest}
		\end{array}$}
\mathopen{\resizebox{1.2\width}{\ht0}{$\Bigg\langle$}}
\usebox{0}
\mathclose{\resizebox{1.2\width}{\ht0}{$\Bigg\rangle$}}
\]
\caption{An illicit adjunct AC structure ($\delta$ adjoined to $\beta$)}
	\label{fig:IllicitAdjunctAC}
\end{figure}
When the host-adjunct structure is finally transferred as part of a larger phase, only the host $\beta$ will be labelled.
Neither the pair $\langle\delta,\beta\rangle$, representing $\delta$ adjoined to $\beta$, nor the adjunct $\delta$ itself will undergo labelling at this stage.
As such, both of these objects will be null-labelled at CI, and therefore, interpreted conjunctively.
This is expected for host-adjunct structures, but when we consider the interpretation of $\delta$, we can see an issue with the structure in \cref{fig:IllicitAdjunctAC}.

The adjunct $\delta$ will be null-labelled, as represented in \cref{ex:AdjunctCI}, and therefore interpreted conjunctively as shown in \cref{ex:AdjunctSEM}.
\ex.\label{ex:AdjunctCI} [$_{\emptyset}$ [the woman]$_i$, [$_{\emptyset}$ Prog, [$_\text{VoiceP}$ the woman$_i$ teach her students]]]

\ex.\label{ex:AdjunctSEM} $\textsc{sem}(\delta) = \textsc{sem}(\text{the woman})(e) \& \textsc{sem}(\text{Prog})(e) \& \textsc{sem}(\text{VoiceP})(e)$

So, DP \textit{the woman} and the VoiceP \textit{the woman teach her students} are predicated of the same extra-mental entity $e$.
In other words, there is some entity $e$ which is both \textit{the woman} and an event of the woman teaching her students.
Notice, however, that the two instances of \textit{the woman} in $\delta$ are not distinct SOs but occurrences of a single SO.
Despite the fact that these two expressions are identical, they are interpreted as being predicated of two distinct entities.
The upper copy is predicated of some event $e$, while the lower copy is predicated of some entity $x$ which participates in the event of teaching students.
If these copies are supposed to be identical, it seems like a contradiction to say that they are predicated of distinct entities.
This contradiction, I propose, is the reason that DPs must move from specifiers of adjunct ACs.

If, on the other hand the DP in [Spec, Prog] is a lower copy, then it can either be ignored by the CI system or treated as a variable.
In either case, it will not be treated as a predicate, and therefore cannot be predicated of two distinct sorts of entities.
\subsection{Generalizing the ACC-ing results}
In the previous section, I offered an explanation for the fact that, if an AC is adjoined to a VP, then its subject must move out of the AC.
No part of the explanation, however, depended on any inherent property of the AC, but rather on the fact that the AC is adjoined, and the fact that the subject DP was internally merged in subject position. 
So, this leads us to the generalization in \cref{ex:AdjunctGen}, which I restate schematically in \cref{ex:AdjunctGenSchema}.
\ex.* $\langle \left\{ \text{DP}_{i} \left\{ \dots t_i \dots \right\} \right\}, \text{XP}\rangle$\label{ex:AdjunctGenSchema}\\
($^\textsc{ok}\langle \left\{ t_{i} \left\{ \dots t_i \dots \right\} \right\}, \text{XP}\rangle$)

Note that this seems to be violated by sentences like \cref{ex:AbsoluteClause}, where the modifier AC has its subject \textit{in situ}.
\ex. Her order having arrived late, Kinza was in a sour mood.\label{ex:AbsoluteClause}

The AC in this sentence however, is a sort of topic, and, by hypothesis, topical expressions are merged in a Topic projection rather than adjoined.
So, the structure of \cref{ex:AbsoluteClause} is given in \cref{fig:AbsoluteClause}.
\begin{figure}[h]
	\centering
	\begin{forest}
	    nice empty nodes,
	    sn edges,baseline
	    [$\delta$
		    [$\gamma$
			    [DP$_{\varphi}$[Her order,roof]]
			    [$\beta$
				    [Prog$_{\varphi}$]
				    [vP[be late,roof]]
			    ]
		    ]
		    [$\alpha$
			    [Topic]
			    [TP [Kinza was in a sour mood,roof]]
		    ]
	    ]
	\end{forest}
	\caption{The unlabelled structure of \cref{ex:AbsoluteClause}}
	\label{fig:AbsoluteClause}
\end{figure}

The DP \textit{her order} and Prog will Agree, meaning the AC $\gamma$ will be labelled $\langle\varphi,\varphi\rangle$, and its constituent $\beta$ will be labelled Prog.
The process of labelling $\delta$ is a slightly more complicated case, though.
Recall that the labelling algorithm is the process of finding the most prominent element in a syntactic object.
Setting aside the cases that result in a feature-pair label, the most prominent element is the least embedded atomic object.
So, what is the least embedded atomic object in $\delta$?
The likely candidates are Topic, Prog, or D (\textit{her}), of which Topic is the least embedded.
Prog is dominated by 3 nodes ($\beta,\gamma,\delta$), as is D (DP,$\gamma,\delta$).
Topic, on the other hand, is dominated by 2 nodes ($\alpha,\delta$).
Therefore, the label of $\delta$ is Topic, and, since \cref{ex:AbsoluteClause} is grammatical, we can safely assume that Topic is strong enough to be a label.
It follows, then, that the label of $\beta$ (a head-phrase structure) will also be Topic.
The end result of the labelling process, then, is represented in \cref{fig:AbsoluteClauseLabel}.
\begin{figure}[h]
	\centering
	\begin{forest}
	    nice empty nodes,
	    sn edges,baseline
	    [Topic
		    [{$\langle\varphi,\varphi\rangle$}
			    [DP$_{\varphi}$[Her order,roof]]
			    [Prog
				    [Prog$_{\varphi}$]
				    [vP[be late,roof]]
			    ]
		    ]
		    [Topic
			    [Topic]
			    [TP [Kinza was in a sour mood,roof]]
		    ]
	    ]
	\end{forest}
	\caption{The labelled structure of \cref{ex:AbsoluteClause}}
	\label{fig:AbsoluteClauseLabel}
\end{figure}

Turning back to resultatives, we can see that the proposed structure included the adjunction of a resP to a VP.
Therefore, the restriction in \cref{ex:AdjunctGenSchema} would apply to resultatives, and the DP in [Spec, res] would be required to move.
Thus we have an explanation for the ungrammaticality of \cref{ex:double-theme-res}. 
\end{document}
