% arara: pdflatex: {options: "-draftmode"}
% arara: biber
% arara: pdflatex: {options: "-draftmode"}
% arara: pdflatex: {options: "-file-line-error-style"}
\documentclass[MilwayThesis]{subfiles}
\begin{document}
In this chapter I will discuss the most recent iteration of Chomsky's syntactic theory, which he refers to as \textit{label theory}.
In the first section I will discuss the background and content of Chomsky's (\citeyear{chomsky2013problems,chomsky2015problems}) proposal.
In the second section I will draw out two questions that Chomsky's label theory leaves unanswered and hypothesize an answer for each.
This modified label theory will then be used to explain the resultative parameter in the following chapter.
\section{Label theory and its motivations}
Chomsky begins his proposal of label theory with a discussion of the minimalist program in general.
In his estimation, the goal of the minimalist program has been to explain the universal properties of language as simply as possible.
The properties he identifies are (i) the structure-dependence of rules, (ii) displacement, (iii) linear order and (iv) projection/labelling.
He then argues that if we assume linear order is a reflex of transfer to the SM interface, properties (i) and (ii) can be explained by assuming that Narrow Syntax is only simplest Merge, as defined in \Next.
\ex. Merge($\alpha$, $\beta$) = $\left\{ \alpha, \beta \right\}$

Unlike previous versions of Merge, however, simplest Merge does not include labelling.
Chomsky argues that this is a welcome outcome, because labelling/projection is not as detectible in surface forms as the other properties of language, and has always been a theory internal notion.
What's more, Chomsky argues, previous theories that bundle labelling with structure building have always stipulated labelling rather than deriving it.
So, for instance, the phrase \textit{see the girl} is stipulated to be a VP rather than a DP.

Chomsky proposes that labels are assigned post-syntactically by a special instance of minimal search called the Labelling Algorithm (LA).
LA operates iteratively in a top-down manner, searching each syntactic object for a ``most prominent'' element which can serve as the label.
In the simplest case, an atomic element (a head X) merged with a complex object (a phrase YP), the atomic element is found to be the most prominent element and, as such, is the label of the object.
\ex. LA$(\left\{ X, YP \right\}) = X$


The head-phrase case in \Last is trivial due to the inherent asymmetry in the structure.
Labelling becomes more complicated when symmetric structures are considered, that is, when head-head and phrase-phrase structures are considered.
To discover how these structures could be labelled, Chomsky considers examples of head-head and phrase-phrase structures that are generated by grammars, and hypothesizes why they are generated and not other instances.
The only head-head structures that surface are those that result from the merger of acategorial roots and category-determining heads.
So, structures like \Next[a] are labellable, but those like \Next[b] and \Next[c] are not.
\ex.
\a. $\left\{ n, \sqrt{\textsc{water}}\right\}$
\b.* $\left\{ n, v\right\}$
\c.* $\left\{ \sqrt{\textsc{ice}}, \sqrt{\textsc{water}} \right\}$

Chomsky proposes that roots are completely featureless and, therefore, invisible to LA, meaning \Last[a] recieves the label $n$.

As for phrase-phrase structures, Chomsky identifies two types that are able to surface.
The first type are what I will call phrase-trace structures.
These are phrase-phrase structure in which one of the constituent phrases is a lower copy.
Following \textcite{moro2000dynamic}, Chomsky proposes that lower copies are invisible to LA, meaning the labeling of a phrase-trace structure is as shown in \Next.
\ex. LA$(\left\{ XP, \langle YP\rangle \right\}) =$ LA$(XP)$\\
(Angle brackets here indicate that YP is a lower copy.)

The second type of phrase-phrase structure that can surface are what I will call agreement structures.
These are phrase-phrase structures in which the two constituent phrases agree with one another for some feature.
In these cases, the agreeing features serve as the label of the structure as in \Next.
\ex. LA$(\left\{ XP_F, YP_F \right\}) = \langle F,F\rangle$

If either of these two situations do not obtain for a given phrase-phrase structure, it will be unlabelable and result in a crash at the CI interface.
To see how this works, consider the raising construction in \Next and the ungrammatical version of it in \NNext.
\ex.\label{ex:raising}
\a. The dishes seem to be dirty.
\b. [$_\alpha$ The dishes [ seem [$_\beta \langle\text{the dishes}\rangle$, [ to be dirty]]]]

\ex.\label{ex:noraising}
\a.* It seems the dishes to be dirty
\b. [$_\gamma$ It [ seem [$_\delta$ the dishes, [ to be dirty]]]]

The sentence in \LLast has two relevant phrase-phrase structures which are labelable.
The first is a trace-phrase structure, given in \Next[a], which is labeled by the infinitive \textit{to}, as demonstrated in \Next[b].
\ex.
\a.  $\beta = \left\{ \langle\text{the dishes}\rangle, \left\{ \text{to}, \text{be dirty} \right\} \right\}$
\b. LA$(\beta) = \text{LA}(\left\{ \text{to}, \text{be dirty} \right\}) =$ to

The second is the agreement structure, given in \Next[b], which is labeled by the agreeing $\varphi$ features as in \Next[b].
\ex.
\a. $\alpha = \left\{ \text{the}_\varphi \text{ dishes} \left\{ T_\varphi \text{, seem} \left\{ \ldots \right\} \right\} \right\}$
\b. LA$(\alpha) = \langle\varphi, \varphi\rangle$

The derivation of \ref{ex:noraising}, however, crashes because the phrase-phrase structure, $\delta$ is unlabelable.
The DP \textit{the dishes} has not raised, so it is visible to LA in $\delta$, and there is no $\varphi$-agreement between \textit{the}$_\varphi$ and \textit{to}$_\emptyset$.
\ex.
\a. $\delta = \left\{ \text{the}_\varphi \text{ dishes} \left\{ \text{to}_\emptyset \left\{ \ldots \right\}\right\} \right\}$
\b. LA$(\delta) = $ Undefined


Also, Chomsky proposes that heads that bear only a partial set of features (\textit{e.g.} English finite T$_\varphi$) cannot label unless they agree for those features with some other head.
This is in contrast to heads that bear full feature sets (\textit{e.g.} Italian T${\langle\varphi,\varphi\rangle}$) or lack these features altogether (\textit{e.g.} English non-finite T$_\emptyset$) which are able to label without agreement.

At this point I should note an issue that arises when giving label-based explanations of syntactic derivations.
In previous theories, movement operations were described as being ``driven'' by some need.
For example, in Government and Binding theories DPs undergo movement in order to get abstract Case.
In minimalist theories, this has been generalized such that all movement is driven by the need to satisfy some feature.
This led debates around the mechanism for driving movement. 
Greed-based accounts, for instance, argue that an object moves if and only if such a move will satisfy one of its own features, while those that assume Enlightened Self Interest take the weaker stance that an object moves if and only if such a move will satisfy some feature on some argument.\footnote{
	See \textcite{lasnik1999last} for a discussion of these two types of accounts.
}
While these accounts were all based on an interface-based theory, they allow syntacticians to explain (un)grammaticality purely in terms of narrow syntax.

Label theory, however, assumes that all operations are free, that is, they don't require a trigger or a driver.
This means, however, that an explanation of why an operation occurs or does not occur in a given derivation is slightly more complicated in label theory.
The well-formedness of a structure is assessed at the interface, which means entire phases are assessed at once.
Consider, for instance, the successive \textit{wh}-movement in \Next, and how the two types of theories would account for it.
\ex. Who$_i$ does Mary say $t_i$ that Laura likes $t_i$?

The accounts of the final movement step ([Spec, C] to [Spec, C]) would be similar, as both theories assume that the highest C needs to agree with a \textit{wh}-word, either for labelling or feature-satisfaction.
The explanations of the first movement ([Comp, V] to [Spec, C]) however, are different.
If movement operations must be driven, then we would likely need to posit a feature on the lower C which must be satisfied by a \textit{wh}-word.
With free movement, however, the \textit{wh}-word must move to the lower [Spec, C] because, if it doesn't, it cannot move to the higher [Spec, C] without violating Subjacency.
Assuming a phase-based theory of subjacency, the first movement operation in \Last follows from the proposal that C is a phase head.

To summarize, a syntactic derivation in label theory proceeds as follows.
Structures are built by iteratively applying Merge (along with Select and Copy) to syntactic objects.
At certain points a portion of a structure (\textit{i.e.}, a phase) is transferred to the interfaces.
At the CI interface, the labelling algorithm labels the transferred structure and all of the structures contained within the transferred structure.
If the labelling algorithm fails to label any part of the transferred structure, the derivation crashes.
A summary of the labelling algorithm is given below in \Next.
\begin{table}
	\centering
	\begin{tabular}[t]{llp{5cm}}
		\textbf{SO} & \textbf{LA(SO)} & \\
		\cline{1-2}
		$X$ & $X$ & ($X$ is not a root, and does not have an incomplete feature set)\\
		$\left\{ X, R \right\}$ & LA$(X)$ & ($R$ is a root, $X$ is not a root)\\
		$\left\{ X, YP \right\}$ & LA$(X)$ & \\
		$\left\{ \langle XP\rangle, YP \right\}$ & LA$(YP)$ & \\
		$\left\{ XP_F, YP_F \right\}$ & $\langle F,F\rangle$ & ($XP$ and $YP$ agree for $F$)\\
		Otherwise & Undefined &
	\end{tabular}
	\caption{A summary of the labelling algorithm}
	\label{tab:LA-results}
\end{table}

\textcite{chomsky2015problems} demonstrates that label theory has some empirical advantage over previous theories of syntax, but leaves at least two questions unanswered.
The first question is why labels would be required by the CI interface at all, and the second question is how are host-adjunct structures labelled.
In the next section I will propose answers to those questions, before considering in the following section the ramifications label theory has for the architecture of the grammar.

\section{Modifications to label theory}

In this section I will address two questions which \textcite{chomsky2013problems,chomsky2015problems} largely leaves open.
First there is the question of how to label Host-Adjunct structures.
These are cases of XP-YP structures, but generally involve neither movement of host or adjunct, nor agreement between the two.
Chomsky's LA, then, would crash when processing these structures.
I propose, following \textcite{hornstein2009theory} and \textcite{chametzky1996theory}, that Host-Adjunct structures are unlabelled.
How is it that a structure can be unlabelled?
To answer this question we must consider the nature of the labelling process. 

If we consider the labelling process to be a function Label from unlabelled structures to labelled structures, then an unlabelled structure is an impossible output of Label.
If Host-Adjunct structures are unlabelled, then they cannot be the output of Label.
This means one of two things, either Host-Adjunct structures are not interpreted at the CI interface, or they bypass Label.
The first possibility seems to be contradicted by the fact that Adjuncts are interpreted at CI, so I will follow the second possibility.
Consider, then, a structure [XP, ZP], where XP is a host and ZP is an adjunct.
The structure as a whole will bypass Label, but the host XP will be labelled.
It is reasonable to assume, then, that the adjunct ZP will bypass Label, meaning its internal structure will not be labelled in this cycle of Label.\footnote{The astute observer will likely note that this hypothesis contains a significant amount of stipulation.
I address this stipulation in chapter \ref{sec:Conclusion}}

The second question is why labels should be required by the CI interface at all.
My proposed answer is that the label of a complex object determines how that object composes semantically.
While this may seem \textit{ad hoc}, it is actually a quite reasonable hypothesis.
Consider Chomsky's labelling hypothesis as phrased in \Next, and the more standard theory of the CI interface in \NNext.
\ex. A syntactic object is a valid CI object iff it is labellable.

\ex. A syntactic object is a valid CI object iff it composes semantically.

At first glance, these hypotheses are incompatible, giving us three options at resolving the conflict.
The first option would be to reject one of the conflicting hypotheses.
There is no strong evidence, however, to reject either \LLast or \Last, so I will not choose this option.
The second option is to conjoin the iff clauses as in \Next.
\ex. A syntactic object is a valid CI object iff it is labellable and it composes semantically.

This option is unattractive for reasons of theoretical parsimony, so I will not choose it.
The third option is to hypothesize that labelling and composition are identical, and therefore the conflicting hypotheses are equivalent.
We can, then, replace our two conflicting statements with the two compatible statements in \Next and \NNext below.
\ex. A syntactic object composes iff it is labellable.

\ex. A syntactic object is a valid CI object iff it composes.

This move is theoretically attractive partially due to the fact that it mirrors the logic of antisymmetry on the SM interface \parencite{kayne1994antisymmetry}.
In the case of antisymmetry, Kayne identifies asymmetric c-command with linear order, and there is no compelling reason to think that the CI interface should be more complex than the SM interface.

So, what would it mean for labelling and composition to be two sides of the same coin?
Again, it is helpful to consider the SM interface, where asymmetric c-command and linear order are associated because they are isomorphic.
We should expect a similar isomorphism to hold between composition and labels, and, in fact, there seems to be good reason to think that there is such an isomorphism.
Consider the main modes of composition generally assumed by semanticists \parencite[\textit{e.g.}, by][]{heimkratzer1998semantics}, given schematically in \Next.
\ex. 
\a. \textbf{Lexical insertion}\\
\textsc{sem}($\alpha$) = $\alpha^\prime$
\b. \textbf{Function application}\\
\textsc{sem}($\left[ \alpha, \beta \right]$) = \textsc{sem}($\alpha$)(\textsc{sem}($\beta$))
\b. \textbf{Predicate modification}\\
\textsc{sem}($\left[ \alpha, \beta \right]$) = \textsc{sem}$(\alpha)(x) \&$ \textsc{sem}$(\beta)(x)$
\b. \textbf{Predicate abstraction}\\
\textsc{sem}($\left[ \alpha, \beta \right]$) = (Op$x$)(\textsc{sem}($\beta$)($x$))

Each of these modes of composition has a corresponding structure type as identified by label theory, including my modifications thus far.
Lexical insertion operates on a single syntactic atom, \textit{i.e.}, a head, which label theory necessarily distinguishes from other syntactic objects.
Predicate modification is the next most complex operation, it conjoins two (possibly complex) objects without requiring or inducing any ordering of the two, exactly isomorphic with the output of merge: unlabelled and unordered syntactic objects.
Function application, likewise, requires two objects, but these objects are ordered.
Unlike conjunction structures created by predicate modification, which are commutative ($X \& Y = Y \& X$), the function-argument structures created by function application are inherently asymmetric ($X(Y) \neq Y(X)$).
This matches with head-labelled stuctures, which encode a pair of objects (the contents of the structure) and an ordering statement (the label).
Finally, predicate abstraction, which creates structures similar to quantifier structures, requires the content of the two expressions, an ordering between the two, and a variable.
Pair-labelled structures provide this information.

The simplest cases are those structures labelled by heads.
The classes of structures which get head labels are given in \Next.
\ex. \textbf{Head-labelled structures}
\a. $\left\{ \text{X, }\textsc{Root} \right\} \xrightarrow{Label} \left[_\text{X} \text{X, }\textsc{root}  \right]$
\b. $\left\{ \text{X, YP} \right\} \xrightarrow{Label} \left[_\text{X} \text{X, YP} \right]$
\c. $\left\{ t_\text{ZP}, \left\{ \text{X, YP} \right\} \right\}\xrightarrow{Label}\left[_\text{X} t_\text{ZP}, \text{XP} \right]$

I propose that in these cases, the objects compose by function application, with the label being the function and the non-labelling constituent being the argument.
So, for instance, a DP is interpreted as the function D, with NP as an argument.
\ex. \textsc{sem}($\left[_\textit{the} \textit{the, ball} \right]$) = \textsc{sem}(\textit{the})(\textsc{sem}(\textit{ball}))

The next case is that of structures labelled by feature-pairs.
These structures, tend to be the result of internal Merge, which is generally associated with operator-variable structures.
I hypothesize, then, that feature-pair labels signal that a complex object is to be interpreted as an operator-variable structure.
For instance the Wh-question structure in \Next[a] is interpreted as in \Next[b].
\ex. \textsc{sem}($\left[_{\langle Q,Q \rangle} \textit{Who}_Q, \left[ \text{C}_Q+\textit{did}, \left[ \textit{Mary see } t_{Who} \right] \right]  \right]$) = (Wh\textit{x})(\textsc{sem}(\textit{Mary saw x}))

Finally, the case of unlabelled structures, which is identical to the case of Host-Adjunct structures.
In this case, objects compose by conjunction.
Consider the interpretation of the structure in \Next.
\ex. \textsc{sem}($\left[_\emptyset \textit{run, swiftly} \right]$) =  $(\lambda e)(\textsc{sem}(\textit{run})(e) \& \textsc{sem}(\textit{swiftly})(e))$


I have identified Kayne's (\citeyear{kayne1994antisymmetry}) theory of the SM interface as an inspiration for my proposal, and I would like to say a little more about the similiarities between his and my hypotheses.
Rather than positing active process, be it simple or intricate, for linearizing a hierarchical structure, Kayne suggests that linear order is the product of a passive isomorphism.
Since asymmetric c-command \textit{is} a linear, or total, order, a hierarchical structure can be mapped to a linear string based purely on properties of that structure.
The idea that an interface between mental modules should be passive, is in keeping with the very idea of modularity.
If the SM module and the Narrow Syntax module are truly independant, then we would not expect there to be any specialization of one in order to interact with the other.
My proposal for the CI interface is, I believe, a step towards a passive interface.
Although evidence for the nature of the CI module is not as readily available as it is for the nature of the SM module, the working, albeit tacit, assumption seems to be that the CI module deals in representations that are formally very similar to formulas of predicate logics.
If we assume a predicate logic with operators ($\forall$, $\exists$, $Wh$, M,\ldots), functions/predicates ($P, Q, f, g,$\ldots), variables ($x, y, z,$\ldots), and conjunction ($\&$), then we can see how there could be an isomorphism between labelled syntactic objects and formulas of this logic.

While it seems that SM objects are linear structures (strings), CI objects seem to be more complex, including notions of scope and variable binding.
As such, I am not able to give a simple order-theoretic explanation of the CI interface as \textcite{kayne1994antisymmetry} gives for the SM interface.
What I can do, however, is give a rather coarse-grained mapping between labelled SOs and expressions in predicate logic.
The class of labelled SOs and that of complex expressions of predicate logic can each be divided into three subclasses.
Labelled SOs can be null-labelled, head-labelled, or pair-labelled, while expressions of predicate logic can be conjoined expressions, function-argument expressions, or operator expressions.
Furthermore these subclasses seem to map to each other as shown in \Next.
\ex.
\begin{tabular}[t]{ll}
	Labelled SO & Predicate logic\\
	\hline
	\hline
	Null labelled & Conjoined expression\\
	$\left\{ \emptyset, \left\{ \text{run, quickly} \right\} \right\}$ & $\textbf{run}(e) \& \textbf{quick}(e)$\\
	\hline
	Head labelled & Function-argument\\
	$\left\{ \text{in}, \left\{ \text{in, the snow} \right\} \right\}$ & $\textbf{in}(\textbf{the\_snow})$\\
	\hline
	Pair-labelled & Operator expressions\\
	$\left\{ \langle Q,Q\rangle, \left\{\text{Who}_Q, \left\{\text{C}_Q, \text{fell} \right\}  \right\} \right\}$ & $\text{Wh}x(\textbf{fell}(x))$\\
	\hline
\end{tabular}

This mapping is, of course, a first approximation of a theory of the CI interface.
Being a first approximation, it will face empirical and theoretical challenges.
For instance, there are likely cases where an expression's label does not seem to map to its interpretation.
These cases, however, might be cases in which our syntactic or semantic analysis is incorrect.
Assuming the hypothesized mapping in \Last is empirically justified, we would need to explain why that mapping and not some other mapping holds.
This would require us to show that the mapping between, say, head-labelling and function application is as tight as the mapping between asymmetric c-command and linear precedence.
Such a discussion, however, is beyond the scope of this thesis.
\section{Summary}
In this chapter I reviewed Chomsky's (\citeyear{chomsky2013problems,chomsky2015problems}) label theory which says that labels are assigned algorithmically at the CI interface and are required for proper interpretation at that interface.
I identified two questions left open in Chomsky's proposal, which I then suggest answers to.
The first question, how would host-adjunct structures be labelled, was answered by the hypothesis that host-adjunct structures are ignored by the labelling algorithm and are null-labelled.
The second question, why labels are required at the CI interface, has answered with the hypothesis that the label of a structure determines how it composes.
In the next chapter, I will use this modified label theory to demonstrate that the (un)availability of resultative can be derived from the (un)availability of bare stem compounding.
\end{document}
