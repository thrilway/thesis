This dissertation rests on a number of non-standard theoretical assumptions and draws a few non-standard distinctions, which I will defend in this chapter.
My defence of the assumptions, however, will not be an argument that they are true, as the truth of any theoretical statement ultimately depends on the empirical facts.
Rather, my defence will actually be an offence; I will argue that the standard assumption is, in fact, ill-founded.
So, in a sense, I will be rejecting standard assumptions rather than making non-standard ones.
So, I will identify problems with the standard assumptions, and present non-standard assumptions which do not face those problems.
The distinctions I draw, in contrast, will not be defended, but rather explained and clarified.

\section{The $\Theta$-Criterion}\label{sec:theta-crit}
The $\theta$-criterion as standardly assumed was first formulated by Chomsky in \textit{Lectures on Government and Binding} (LGB) as \Next.
\ex. Each argument bears one and only one $\theta$-role, and each $\theta$-role is assigned to one and only one argument. \parencite[36]{chomsky1981lectures}

In a footnote, Chomsky justifies this criterion, saying 
\begin{quote}
	The second clause of [the $\theta$-criterion] is well-motivated.
	To say that each $\theta$-role must be filled implies, for example, that a pure transitive verb such as \textit{hit} must have an object, that a verb such as \textit{put} or \textit{keep} (with the sense they have in \textit{put it in the corner}, \textit{keep it in the garage}) must have the associated PP slot filled, etc. 
	The additional requirement that each $\theta$-role must be filled by only one argument will, for example, exclude the possibility that a single trace is associated with several argument antecedents, a possibility ruled out in principle under the Move-$\alpha$ theory. 
	\parencite[139]{chomsky1981lectures}
\end{quote}
I would agree that the second clause of \Last, that each $\theta$-role is assigned to a single argument, is well-motivated by the empirical considerations Chomsky cites, and as such I will not reject that portion of the $\theta$-criterion.
The first clause, however, is motivated mainly by theoretical concerns of LGB, that is, its empirical motivation is indirect at best.

The nature of the LGB theory is such that its various hypotheses and principles are connected to each other in a web-like network.
As a result, the first clause of the $\theta$-criterion depends on various other theoretical statements and various other theoretical statements depend on it.
So, rather than attempting an exhaustive enumeration of the links between the $\theta$-criterion and the other theoretical statements of LGB, I will present what I consider to be the best argument in favour of the $\theta$-criterion and argue that its premises have since been rejected within syntactic theory.

The first premise is the now-familiar Y- or T-model of grammar shown in \autoref{fig:LGBYModel}, which LGB theory continues from earlier theories.
According to this model, a syntactic derivation has four levels of representation (D-structure, S-Structure, PF, and LF), and each step in the derivation is performed by the application of a subset of the transformational rules.
S-structures are derived by the applying of Move-$\alpha$ to D-structures, LFs are derived by applying QR (and maybe Move-$\alpha$) to S-structures, and PFs are derived by applying ``stylistic rules'' to S-structures \parencite[18]{chomsky1981lectures}.
\begin{figure}[h]
	\centering
\begin{tikzpicture}[baseline]
	\node[draw,rounded corners] (DS) at (0,0) {D-Structure};
	\node[draw,rounded corners] (SS) at (0,-2) {S-Structure};
	\node (PF) at (-2,-3) {PF};
	\node (LF) at (2,-3) {LF};
	\draw[->] (DS)--(SS) node[midway,right] {Move-$\alpha$};
	\draw[->] (SS)--(PF) node[midway,anchor=south east] {Stylistic rules};
	\draw[->] (SS)--(LF) node[midway,anchor=south west] {QR};
\end{tikzpicture}
	\caption{The architecture of the grammar in LGB}
	\label{fig:LGBYModel}
\end{figure}
The exact formal properties of the transformations are not important for this discussion.
What is important, is that all syntactic displacement is the result of one of these transformations.

The second premise is the projection principle, which states that lexical properties must be represented at all levels of syntax.
Since $\theta$-roles are lexical properties of (at least) verbs, they must be represented at all levels of syntax.
Consider the verb \textit{hit}, whose lexical entry specifies that it needs a patient argument.
The projection principle requires that at D-Structure and S-Structure \textit{hit} must have assigned a patient $\theta$-role to an argument and therefore, assuming the patient $\theta$-role is assigned to Comp,V, there must be a nominal in the complement position of \textit{hit} at both D-Structure and S-Structure.

With these assumptions, it follows that no single argument can receive more than one $\theta$-role.
Suppose there is a derivation in which a single argument X receives two $\theta$-roles $\Theta1$ and $\Theta2$.
According to the projection principle, X must be marked with both $\Theta1$ and $\Theta2$ at D-Structure.
Since each $\theta$-role is associated with a unique structural position, it follows that X must be in two distinct positions at D-Structure.
The only way an argument can be in multiple positions is if it has undergone Move-$\alpha$.
Move-$\alpha$, however, maps D-Structures to S-Structures.
Therefore, an argument cannot be in two positions at D-Structure, and furthermore, cannot be multiply $\theta$-marked at D-Structure.
If an argument cannot be multiply $\theta$-marked at D-Structure, then it cannot be multiply $\theta$-marked at all.

Thus we are able to derive the first clause of the $\theta$-criterion from other principles.
These principles, however, have either been rejected or problematized since their statement in LGB.
Since \textit{The Minimalist Program} \parencite{chomsky1995minimalist}, generative theories have largely dispensed with D-Structure and S-Structure.
Without these levels of representation, the projection principle (as formulated in LGB) is effectively meaningless, and without the projection principle, there is no more basis for the first clause of the $\theta$-criterion.\footnote{
	Note that this is far from a knock-down argument against the first clause of the $\theta$-criterion.
	In fact, the $\theta$-criterion, as it is expressed in LGB may still hold, but in order to show this, we need empirical arguments.
}
Therefore, I will not assume the first clause of the $\theta$-criterion.

\section{Last Resort}
The goal of the minimalist program is to clean up syntactic theory by explaining unnecessary principles in terms of necessary principles.
In nearly one fell swoop, \textcite{chomsky1995minimalist} eliminates a number of the complications built into LGB theory (S-Structure and D-Structure chief among them).
In \textit{The Minimalist Program} (MP), however, Chomsky was not able to explain two apparent imperfections: displacement and uninterpretable features.\footnote{
	The framework developed in MP also does not explain projection/labelling, but Chomsky does not recognize this as an imperfection until ``Problems of projection'' \parencite{chomsky2013problems}.
	More on this in Chapter \ref{sec:labels}.
}
Chomsky formalized displacement as the operation Move, which is more complex than Merge, and proposes that, unlike Merge, which ``comes for free,'' every instance of Move must be triggered by the need to satisfy an uninterpretable feature.
The simple operation Merge is preferred for economy reasons, while movement, Chomsky argues, is a ``last resort.''
In work subsequent to MP, however, Chomsky proposes that Move is actually a sub-type of Merge, called Internal Merge.
Merge and Move are different names for the same operation, there is no reason to think that Move is more computationally complex, and therefore there is no reason to think that movement is a last resort.

In ``Beyond Explanatory Adequacy'' \parencite{chomsky2004beyond} Chomsky makes this rejection of Last Resort explicit:
\begin{quote}
	[Narrow Syntax] is based on the free operation Merge.
	[The Strong Minimalist Thesis] entails that Merge of $\alpha$, $\beta$ is unconstrained, therefore either external or internal.
	Under external Merge, $\alpha$ and $\beta$ are separate objects; 
	under internal Merge, one is part of the other, and Merge yields the property of ``displacement,'' which is ubiquitous in language and must be captured in some manner in any theory. 
	It is hard to think of a simpler approach than allowing	internal Merge (a grammatical transformation), an operation that is freely available there is.
	\parencite[110]{chomsky2004beyond}
\end{quote}
This line of reasoning bears discussion in light of the fact that Last Resort is still standardly assumed by self-described minimalist syntacticians.
In fact, several syntacticians expand Last Resort to External Merge arguing that neither type of Merge ``comes free'' \parencite{pesetsky2006probes,frampton2008crash,wurmbrand2014merge,yokoyama2015features}.
This proposal is understandable from a historical perspective, but ultimately misguided in my opinion.

Perhaps the most attractive aspect of a constrained Merge syntax is the purported gains in computational efficiency.
To illustrate this, \textcite{frampton2008crash} consider the incomplete product of a doomed derivation in \Next.
\ex. it to be believed Max to be happy

In a free Merge syntax, according to \textcite{frampton2008crash}, the derivation must continue for an indefinite time until a phase head is merged.
At this point, the derivation will crash due to a Case Filter violation.
A constrained Merge syntax, however, could halt as soon as the derivation becomes doomed, say, when \textit{it} is merged.
This would save us the indefinite number of steps it takes to merge a phase head and is, therefore, more efficient.
This I take to be a species of the argument that free Merge systems are inefficient because, in addition to the infinite array of convergent derivations they must generate, they also generate an infinite array of crashing derivations, whereas constrained Merge systems only generate to ``convergent'' derivations.
However, as I show below, when we investigate the nature of constrained Merge, the purported gains in efficiency in one part of the system come at the expense of another part of that same system.
That is, a system with constrained Merge is not inherently more efficient than a system with free Merge, and since there is no efficiency difference between the two systems, the choice between them will have to be based on some other metric.

At a minimum, abstracting away from Agree operations, each version of the syntax will have a Merge operation and a Transfer operation.
So, the free Merge syntax will consist of an unconstrained Merge$_F$ and a constrained Transfer$_F$ as defined in \Next.
\ex.
\a. Merge$_F(\alpha,\beta) = \left\{ \alpha, \beta \right\}$ 
\b. Transfer$_F(\gamma) = \langle\textsc{sem}(\gamma), \textsc{phon}(\gamma)\rangle$ iff Filter($\gamma$) = F\\
(where $\alpha$, $\beta$, and $\gamma$ are syntactic objects.)

A constrained Merge syntax, then, would consist of a constrained Merge$_C$ and an unconstrained Transfer$_C$.
\ex.
\a. Merge$_C(\alpha,\beta) =\left\{\alpha,\beta\right\}$ iff Satisfy($\alpha,\beta$) = T
\b. Transfer$_C(\gamma) = \langle\textsc{sem}(\gamma), \textsc{phon}(\gamma)\rangle$\\
(where $\alpha$, $\beta$, and $\gamma$ are syntactic objects.)

If we assume that both theories can be made descriptively adequate, then the Satisfy predicate will have the same net effect as the Filter predicate required for the free Merge system.
So, given any pair of syntactic objects $\alpha$ and $\beta$, Satisfy must be able to evaluate whether Merge($\alpha,\beta$) is allowed.
And since there are an infinite number of derivable syntactic objects, even in a constrained Merge syntax, Satisfy must be able to evaluate an infinity of possible $\beta$'s against each possible $\alpha$.
For any given syntactic object, then, there is an indefinite number of objects which will merge with that object and an indefinite number that will not, and the only way to know if Satisfy is true of a pair of objects is to check.
So, much like free Merge syntax suffers from an infinity of crashes, constrained Merge syntax suffers from an infinity of failed Merge operations.

A constrained Merge theorist might still object by saying that Satisfy is a local operation, while Filter is a global operation, and local operations are to be preferred if we care about computational complexity.
Again, this is an intuitively attractive argument, but not obviously valid.
Consider the following thought experiment.
Suppose you are a TA charged with grading a quiz by your somewhat maniacal instructor.
Part of the instructor's mania is that they require all quizzes to consist of 40 equally weighted questions and be graded out of 10 points.
What is the most efficient procedure for assigning a grade to each quiz?
Two types of procedure suggest themselves.
The first, which I will call the Local Only procedure, is to assign each correct answer a value of 0.25 points and then add up all of the points.
The second, which I will call the Local/Global procedure, is to assign each correct answer a value of 1 point, add up all of the points, and divide by 4 (perhaps with a calculator).
Since humans are very good at counting by increments of 1, and calculators are very good at dividing by 4, while neither is very good at counting by increments of 0.25, the Local/Global procedure is likely to be more efficient than the Local Only procedure.
The moral of this story: One machine's global procedure is another machine's local procedure.

There is also a methodological rationale for preferring a free Merge framework which is slightly counterintuitive so I would like to dwell on it for a moment.
My reason for assuming a free Merge framework is that it creates, or rather, lays bare, more problems for us to solve.
Why is this preferable?
Shouldn't we prefer the theory with fewer problems?
Intuitively, we should prefer the less problematic theory, but this depends on how we count a theory's problems.
I would like to argue that, while free Merge theories pose a greater number of problems than constrained Merge theories, the sheer weight of the problems posed by each type of theory is equal to that of the other.
Furthermore, the problems of free Merge can be made into empirical questions more readily than those of constrained Merge.

In order to argue in favour of free Merge, I will present one argument against it and show how that argument actually strengthens my claims in the previous paragraph.
The argument comes from \textcite{frampton2008crash} and states that a free Merge theory of grammar must posit ``filters'' to rule out the non-converging structures that its syntax generates, and the last thing we want is a flourishing of filters.
I could not agree more with their assessment, but where they see a bug, I see a feature.
So-called ``filters'' are not attempts at explanation, but descriptions of generalizations in need of explanation.

Take, for instance, the remaining clause of the $\theta$-criterion, given in \Next.
\ex. [E]ach $\theta$-role is assigned to one and only one argument. \parencite[36]{chomsky1981lectures}

To propose a $\theta$-filter, then, would be to say that those derivations that violate \Last crash at an interface, presumably the CI interface.
For a theorist, this filter is actually a question or series of questions: Why is it that only those derived structures that satisfy \Last are valid CI objects?
That question may not be empirical, but it invites hypotheses which may lead to empirical questions that we don't currently know how to ask.
Very likely, due to the interface nature of the questions, their answers will not be narrowly linguistic.

Now, consider the situation constrained Merge puts the theorist in.
For \textcite{frampton2008crash}, the $\theta$-criterion is expressed by ``selectional features'' on heads which must be satisfied immediately.
This leads to a number of questions: What is the nature of these selectional features?
How are they related to, say, $\varphi$-features?
Do they exist independently of the narrow syntax?
Why do they need to be satisfied first?
And so on.
I, for one, don't have the slightest clue how to proceed in answering or even sharpening these questions, and there don't seem to be any clues in the offing from constrained Merge theorists.
\section{Long Distance Agree}
Strongly associated with, but distinct from Last Resort is the notion of an Agree operation.
\textcite[101]{chomsky2000minimalist} introduces Agree as an operation ``which establishes a relation (agreement, Case checking) between an LI $\alpha$ and a feature F in some restricted search space (its \textit{domain})'', and virtually every syntactician who identifies as a minimalist assumes that such an operation is part of the syntactic computation.
In fact, there is an ongoing debate as to the exact nature of Agree \parencite{zeijlstra2012there,preminger2013thats,bjorkman2014upward}, but there is nonetheless a broad consensus with respect to the general properties of Agree.
All of the versions of Agree have three components which I will call Probe, Match and Satisfy.
If we take Agree to operate on a head H and a feature F, then Agree states that F Satisfies H if and only if H Probes and Matches F.\footnote{
	Depending on the theory, there may be additional requirements placed on what sorts of heads and features can undergo Agree.
	For instance, many theories assume that H or F must be active to undergo Agree, where the precise definition of \textit{active} depends on the theory in which it is situated.  
}
Probe is a predicate that evaluates to true if and only if a structural relation R holds between H and F.
In virtually all versions of Agree, R is some combination of c-command and (relativized) minimality.
The versions tend to differ on the directionality of R, that is, whether H c-commands F or vice-versa.
Match is a predicate that evaluates to true iff H bears some feature of the same type as F.
So, a head bearing unsatisfied $\varphi$-features will Match $\varphi$-features.
If Probe and Match succeed, then F can Satisfy H, where satisfaction is generally taken to be feature valuation.
So, a copula bears unvalued $\varphi$-features which are valued by the inherent $\varphi$-features of a DP.

The prototypical evidence for this conception of Agree comes from morphological agreement in existential constructions like \cref{ex:ExistAgr}, where the verb agrees with a DP in its c-command domain rather than the expletive \textit{there} in its specifier.
\ex.\label{ex:ExistAgr} There are three plates in the sink.

As I see it, though, the main reason for keeping long-distance Agree in the Narrow syntax is that it seems to save Last Resort theories of movement from a rather vicious look-ahead problem.
As I discussed in the previous section, the earliest version of Last Resort held that a movement operation proceeds only if it satisfies some uninterpretable feature.
Under this theory, however, feature satisfaction occurs only as a result of movement, which means that in order to know if a movement operation satisfies a feature, we need to perform that movement.
This has the effect of saying that Move($\alpha$) is defined iff Move($\alpha$) is defined.
If instead, Move is preconditioned on Agree, then we no longer have an infinite regress.

Within the group of minimalist syntacticians, there is a somewhat disparate minority that rejects the orthodoxy of Long Distance Agree for a number of reasons.
In the remainder of this section I will discuss the main theoretical\footnote{
	\textcite{bobaljik2008wheres,arregi2013contextual} present \textcite{empirical} rather than \textit{theoretical} arguments in favour of a post-syntactic conception of Agree, but their data and analyses are too involved to do justice in this section. 
} argument against a narrow syntactic notion of Agree, which is that this notion of Agree introduces a redundancy into the language faculty which violates the scientific principle of parsimony.

\textcite{hornstein2009theory} argues that Agree is redundant with Merge and should, therefore, be rejected as an operation of the Narrow Syntax.
Once Move is unified with Merge in the form of Internal Merge, then Merge becomes an operation capable of creating non-local dependencies.
Since the creation of non-local dependencies is the sole purpose of Agree, the standard minimalist model of grammar has two operations that create non-local dependencies.
This would not be an issue, however, if the types of dependencies created by Merge were distinct from those created by Agree, but they seem to be nearly identical.
As I discussed above, each instance of Agree is restricted to the c-command domain of some head, with an added (relativized) minimality restriction.
Therefore, the dependencies created by Agree are restricted to those that obey c-command and (relativized) minimality.
Dependencies created by Merge have exactly the same restrictions:
C-command is definable in terms of Merge, and minimality is traditionally a restriction on movement.
The only difference between Agree dependencies and I-Merge dependencies is that Agree dependencies don't appear to show displacement, while I-Merge ones do.
\textcite{hornstein2009theory} argues that this is not enough of a difference to justify an entirely distinct operation.
Rather than assuming a novel operation, we can just take Agree to be a variety of movement (and therefore a variety of Merge) in which the lower copy is pronounced.
Doing this would eliminate a redundancy, and push our study of agreement to the interfaces, a result I believe to be theoretically attractive.

Of course, theoretical attractiveness is not enough to justify an assertion as true.
Only accordance with the facts can justify a theoretical claim.
This is difficult for a negative claim, such as the rejection of long-distance Agree, since no amount of observation or exploration can definitively justify it.
Rather, the task of the minimalist, given theory T which accounts for the set of facts C, is to show that a simpler theory T$^\prime$ can also account for C,\footnote{
As \textcite{chomsky1965aspects} notes, however, the notion of simplicity is not quite empirical so any debate over whether T$^\prime$ is simpler than T will be philosophical rather than scientific.
I will take for granted that a theory of syntactic derivations without long-distance Agree is simpler than one with it, all else being equal.}
 and the only way to demonstrate this is to assume the simpler theory and show how it is able to explain the facts in question.
That will be my strategy in this thesis: I will simply assume that there is no Long Distance Agree in the Narrow Syntax, and go about explaining the facts in question with the simplified theory.
\section{Terminological notes}
The subject matter of this thesis is often called the ``syntax-semantics interface,'' a term which I have discovered is ambiguous, probably due to the fact that it is constructed from three ambiguous terms.

The term \textit{syntax} has (at least) three senses which seem to be used in generative grammar circles.
The first sense, which I will call the sociological sense, is that \textit{syntax} is what syntacticians do.
For instance, $\theta$-theory belongs to the domain of syntax under this sense, because syntacticians care about it, while semanticists tend not to.
However, $\theta$-theory deals at least partially with meaning so it, at least, intersects with semantics.
This sense would be useful if this dissertation were an intellectual history of generative syntax, but since this is a work of syntactic theory, I will not use this sense.

The second sense, which I will call the broad sense, is that \textit{syntax} is the study (or description) of the form and arrangement of symbolic representations.
Under this sense, the study of syntax would be a part of the study of logic, programming languages, arithmetic, etc.
Furthermore, \textcite[174]{chomsky2000new}, discussing this sense, argues that most of what we call \textit{semantics} and \textit{phonology} would be classified as syntax under this sense.\footnote{Based on my discussion of this sense with phonologists and semanticists, this may be the most controversial claim Chomsky has ever made.}
This sense will prove useful in this dissertation so I will retain it.

The third sense, which I will call the narrow sense, is that \textit{syntax} is a mental module characterized by a computational procedure that generates an unbounded array of structured form\footnote{I use the term \textit{form} here to refer to all possible expressive modalities of language}-meaning pairs.
This, I believe, is what generative syntacticians mean when they use the term \textit{syntax}.
The ``syntax module'' is one of the objects of study of this thesis so I will retain this sense.

Since both the broad and narrow senses are useful to me, I will need to make a distinction for the sake of clarity.
I will use the term ``Narrow Syntax'' (or NS) to refer to the narrow sense, that is, the hypothesized mental module, and ``syntax'' (and derived terms) to indicate the broad sense.

Similar remarks apply to the term \textit{semantics}, which has at least three senses.
The first sense, as in the case of \textit{syntax}, is the sociological sense: semantics is what semanticists do.
I will not be using this sense for the same reasons as I cited above for the sociological sense of \textit{syntax}.

The broad sense of \textit{semantics} is that of the study of the relation of a symbolic system to some other system.
So, to various degrees, we can talk about the semantics of a logical system, a programming language, a natural language, etc.
I will use this sense only informally when discussing notions of truth and reference associated with an instance or class of natural language expression.

The narrow sense of \textit{semantics} is that of the mental module (or system of modules) associated with computing the meaning of a linguistic expression.
Chomsky often refers to this mental entity as the Conceptual-Intentional (CI) system and stresses that we know very little about it.
Insofar as this thesis makes claims or hypotheses about \textit{semantics}, it makes claims or hypotheses about the CI system.

The final ambiguous term I will discuss is \textit{interface}.
In recent years it has become common within generative linguistics to write papers, hold workshops, and compile books on \textit{the syntax-semantics interface}, but as I mentioned above, the term is ambiguous.
It largely seems to be ambiguous between a \textit{sociological} sense and a \textit{narrow} sense, with the sociological sense dominating discussion.

When used in the sociological sense, the syntax-semantics interface refers to a body of literature that mixes the formalisms and methods used by syntacticians with those used by semanticists.
That is, this type of work makes use of tree diagrams and expressions of typed lambda calculus.
In this sense, the interface is not an object of study per se, but a sub-discipline.

In the narrow sense, the syntax-semantics interface refers to the interface between the Narrow Syntax and the CI system.
Assuming this sense results in very different sorts of analyses compared to the standard analyses, analyses that posit computational procedures rather than merely representing expressions in two ways.
In many ways, however, the term \textit{interface} in its narrow sense is a misnomer, as there are likely no mental objects that we might call interfaces.
As best we can tell, the mind consists of a set of modules and a non-modular central system \parencite{fodor1983modularity,fodor2001mind}.
An interface, then, emerges wherever two modules interact with each other, or perhaps where a module interacts with the central system.
Restricting ourselves to the modules, we can see why positing interfaces as mental objects won't do.
Suppose we have two modules, M1 and M2, which seem to interact with each other.
Being modules, each will consist in a set of computational operations (P1 and P2) defined over a class of syntactically structured (in the broad sense) objects (L1 and L2).
Suppose we posit an interface I1, which consists in an operation P3 that converts objects of L1 into objects of L2.
What is I1, then, but a module that has interfaces with M1 and M2?
If I1 is a module, are its interfaces with M1 and M2 also modules?
If so, then we seem to be stuck with an infinite regress.
If not, then interfaces are a special kind of module, but this would raise further questions with respect to their evolutionary origins.

If there are no mental objects that we might call interfaces, then how are we to study them?
The answer to this question is that to study an interface, we must study the modules associated with that interface with the added assumption that such an interface exists.
So, studying the syntax-semantics interface involves studying the Narrow Syntax and the CI module with the assumption that there is an interface between them.
We will get a glimpse of how such a study would work in chapter \ref{sec:labels}.

\section{Summary}
In this chapter, I have made explicit two of my assumptions which would be considered non-standard among contemporary generative syntacticians.
In particular, I do not assume the $\theta$-criterion as it is commonly stated, and I do assume a free Merge syntax.
I have also clarified some terminology that many take for granted; specifically, I clarified my use of the term \textit{syntax-semantics interface} and its constituent terms.
Now that the reader has a sense of my theoretical assumptions, we can move on to more specific concerns in the following chapters.
